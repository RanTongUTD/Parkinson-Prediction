{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNhbBEOUJYlh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PyTorch implementations of mixed effect models applied to\n",
        "Parkinson's Telemonitoring dataset. (Functional Forward Fix 2 within N-Model Structure)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Removed GroupShuffleSplit as it's replaced by custom R-like split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import defaultdict\n",
        "from collections.abc import Generator\n",
        "from math import log, pi\n",
        "from typing import Any, Dict, Final, Optional, List, Tuple, Type\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "import time # Import time for epoch timing\n",
        "\n",
        "# --- Helper Functions/Classes (Replacements for python_tools) ---\n",
        "\n",
        "# Global dtype for consistency\n",
        "TENSOR_DTYPE = torch.float64\n",
        "\n",
        "# Simplified base class (replace neural.LossModule)\n",
        "class LossModule(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # Basic init, ignores unused iterations if passed\n",
        "        kwargs.pop('iterations', None)\n",
        "        kwargs.pop('input_size', None)\n",
        "        kwargs.pop('output_size', None)\n",
        "        # Pop dtype if passed, subclasses might handle it\n",
        "        kwargs.pop('dtype', None)\n",
        "        if kwargs:\n",
        "            # Suppress warning for hidden_sizes as it's handled by MLP\n",
        "            kwargs.pop('hidden_sizes', None)\n",
        "            if kwargs: # Check again if other kwargs remain\n",
        "                warnings.warn(f\"Unused arguments in LossModule init: {kwargs.keys()}\")\n",
        "\n",
        "    def loss(self, scores, ground_truth, meta, take_mean=True, loss=None):\n",
        "        \"\"\"Calculates the base loss (e.g., MSE).\"\"\"\n",
        "        # Ensure inputs are compatible with MSELoss (usually float)\n",
        "        # Cast to float32 for loss calculation if needed, as MSELoss might prefer it\n",
        "        scores_float = scores.float()\n",
        "        ground_truth_float = ground_truth.float()\n",
        "\n",
        "        if loss is None:\n",
        "            loss_fn = nn.MSELoss(reduction='none')\n",
        "            loss = loss_fn(scores_float, ground_truth_float)\n",
        "\n",
        "        # Return loss, potentially casting back to original dtype if required elsewhere\n",
        "        loss = loss.to(scores.dtype)\n",
        "\n",
        "        if take_mean:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            # Return loss per sample, ensure correct shape\n",
        "            # Sum across output dimensions if > 1\n",
        "            if loss.ndim > 1 and loss.shape[1] > 1:\n",
        "                 return loss.sum(dim=1)\n",
        "            else:\n",
        "                 return loss.view(-1) # Ensure 1D output per sample\n",
        "\n",
        "    def can_jit(self) -> bool:\n",
        "        # Assume models generally cannot be JITted easily unless specifically designed\n",
        "        return False\n",
        "\n",
        "# Simplified MLP (replace neural.MLP)\n",
        "# *** Reverted to original structure with self.network ***\n",
        "class MLP(LossModule):\n",
        "    def __init__(self, input_size, output_size, hidden_sizes=[64, 32], activation=nn.ReLU, dtype=TENSOR_DTYPE, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs) # Pass dtype up if needed\n",
        "        self.dtype = dtype # Store dtype\n",
        "        layers = []\n",
        "        last_size = input_size\n",
        "        for i, hidden_size in enumerate(hidden_sizes):\n",
        "            layers.append(nn.Linear(last_size, hidden_size, dtype=self.dtype))\n",
        "            layers.append(activation())\n",
        "            last_size = hidden_size\n",
        "        # Final layer is part of the main sequence now\n",
        "        layers.append(nn.Linear(last_size, output_size, dtype=self.dtype))\n",
        "        self.network = nn.Sequential(*layers) # Contains all layers\n",
        "\n",
        "    # Removed forward_body as it's not needed\n",
        "    def forward(self, x, meta=None, y=None, dataset=\"\", **fwd_kwargs):\n",
        "        \"\"\"Standard forward pass.\"\"\"\n",
        "        # Ensure input is the correct dtype before passing to the network\n",
        "        x = x.to(self.dtype)\n",
        "        return self.network(x)\n",
        "\n",
        "# Simplified Ensemble (replace neural.Ensemble) - Needed by original NME forward\n",
        "class Ensemble(LossModule):\n",
        "     def __init__(self, model: Type[LossModule], size: int, **kwargs):\n",
        "        # The kwargs passed here *are* the model_kwargs\n",
        "        super().__init__(**kwargs) # Pass unrelated kwargs up if any\n",
        "        # Ensure essential args like input/output size are present in kwargs\n",
        "        if 'input_size' not in kwargs or 'output_size' not in kwargs:\n",
        "             raise ValueError(\"Ensemble needs input_size and output_size in its kwargs\")\n",
        "\n",
        "        # Pass all received kwargs to each model instance\n",
        "        self.models = nn.ModuleList([model(**kwargs) for _ in range(size)])\n",
        "        self.dtype = self.models[0].dtype if size > 0 else TENSOR_DTYPE # Infer dtype\n",
        "\n",
        "     def forward(self, x, meta: dict, y: Optional[torch.Tensor] = None, dataset: str = \"\", weights: Optional[torch.Tensor] = None):\n",
        "        # This forward needs to handle the weighting logic as expected by NME\n",
        "        batch_size = x.shape[0]\n",
        "        # Infer output size and dtype from the first model\n",
        "        # *** Access network attribute correctly ***\n",
        "        first_model_last_layer = self.models[0].network[-1] # Assumes MLP structure\n",
        "        output_size = first_model_last_layer.out_features\n",
        "        dtype = first_model_last_layer.weight.dtype # Get dtype from model params\n",
        "\n",
        "        outputs = torch.zeros(batch_size, output_size, device=x.device, dtype=dtype)\n",
        "\n",
        "        if weights is None:\n",
        "            raise ValueError(\"Ensemble forward called without weights.\")\n",
        "\n",
        "        # Aggregate outputs based on weights\n",
        "        # weights shape: (batch_size, num_models)\n",
        "        for i, model in enumerate(self.models):\n",
        "            model_output = model(x) # Shape: (batch_size, output_size)\n",
        "            # Element-wise multiplication and sum across models\n",
        "            # Ensure weights are the correct dtype for multiplication\n",
        "            outputs += weights[:, i].unsqueeze(1).to(dtype) * model_output\n",
        "\n",
        "        # NME expects (scores, meta) - the original Ensemble likely just returned scores\n",
        "        # Let's return scores, meta to match previous structure expectations if needed\n",
        "        return outputs, meta\n",
        "\n",
        "\n",
        "# Cholesky decomposition (replace neural.cholesky)\n",
        "def cholesky(A: torch.Tensor, dtype: torch.dtype = TENSOR_DTYPE) -> torch.Tensor:\n",
        "    # Use torch.linalg.cholesky\n",
        "    # Ensure input is the specified dtype (usually float64) for stability\n",
        "    return torch.linalg.cholesky(A.to(dtype))\n",
        "\n",
        "# Unique with index (replace neural.unique_with_index)\n",
        "def unique_with_index(ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    unique_ids, inverse_indices = torch.unique(ids, return_inverse=True)\n",
        "    # Need the index of the *first* occurrence of each unique id in the original tensor\n",
        "    first_occurrence_indices = []\n",
        "    seen_ids = set()\n",
        "    original_indices = torch.arange(len(ids), device=ids.device)\n",
        "    for id_val, orig_idx in zip(ids, original_indices):\n",
        "        item_id = id_val.item()\n",
        "        if item_id not in seen_ids:\n",
        "            first_occurrence_indices.append(orig_idx)\n",
        "            seen_ids.add(item_id)\n",
        "\n",
        "    # Ensure the order matches torch.unique output order\n",
        "    # Handle potential empty first_occurrence_indices if ids is empty\n",
        "    if not first_occurrence_indices:\n",
        "         first_occurrence_tensor = torch.empty(0, dtype=torch.long, device=ids.device)\n",
        "    else:\n",
        "         first_occurrence_tensor = torch.tensor(first_occurrence_indices, device=ids.device, dtype=torch.long)\n",
        "\n",
        "    # Handle case where unique_ids might be empty\n",
        "    if unique_ids.numel() == 0:\n",
        "        return unique_ids, torch.empty(0, dtype=torch.long, device=ids.device)\n",
        "\n",
        "    ordered_first_indices = {id_val.item(): idx for id_val, idx in zip(ids[first_occurrence_tensor], first_occurrence_tensor)}\n",
        "    final_indices = [ordered_first_indices[uid.item()] for uid in unique_ids]\n",
        "    return unique_ids, torch.tensor(final_indices, device=ids.device, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Covariance block calculation (replace neural.cov_block)\n",
        "def cov_block(\n",
        "    data: torch.Tensor,\n",
        "    independent_indices: torch.Tensor,\n",
        "    full_indices_list: List[torch.Tensor],\n",
        ") -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Calculate variance for independent columns and covariance matrices for blocks.\n",
        "    Args:\n",
        "        data: Tensor of shape (n_samples, n_features), expected dtype TENSOR_DTYPE\n",
        "        independent_indices: Indices of columns assumed independent.\n",
        "        full_indices_list: List of tensors, each containing indices for a covariance block.\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - Variances for independent columns (1D Tensor)\n",
        "        - List of covariance matrices for the specified blocks\n",
        "    \"\"\"\n",
        "    n_samples = data.shape[0]\n",
        "    dtype = data.dtype # Use dtype from input data\n",
        "\n",
        "    if n_samples <= 1:\n",
        "        # Handle edge case: cannot compute covariance with <= 1 sample\n",
        "        num_independent = len(independent_indices)\n",
        "        variances = torch.ones(num_independent, device=data.device, dtype=dtype) # Default to 1\n",
        "        cov_matrices = []\n",
        "        for indices in full_indices_list:\n",
        "            if indices is None or len(indices) == 0: continue # Skip empty blocks\n",
        "            num_features = len(indices)\n",
        "            cov_matrices.append(torch.eye(num_features, device=data.device, dtype=dtype)) # Default to identity\n",
        "        return variances, cov_matrices\n",
        "\n",
        "    # Center the data\n",
        "    mean = torch.mean(data, dim=0, keepdim=True)\n",
        "    centered_data = data - mean\n",
        "\n",
        "    # Calculate variances for independent columns\n",
        "    variances = torch.zeros(len(independent_indices), device=data.device, dtype=dtype)\n",
        "    if len(independent_indices) > 0:\n",
        "         variances = torch.var(data[:, independent_indices], dim=0, unbiased=True)\n",
        "\n",
        "\n",
        "    # Calculate covariance matrices for blocks\n",
        "    cov_matrices = []\n",
        "    for indices in full_indices_list:\n",
        "        if indices is None or len(indices) == 0: continue # Skip empty blocks\n",
        "        # Calculate covariance: (X^T * X) / (n - 1)\n",
        "        block_data = centered_data[:, indices]\n",
        "        # Use torch.cov if available and suitable, otherwise manual calculation\n",
        "        # Note: torch.cov expects features in rows by default, so transpose\n",
        "        if hasattr(torch, 'cov'):\n",
        "             # torch.cov returns scalar (0D) for 1 feature, matrix (2D) otherwise\n",
        "            cov_matrix = torch.cov(block_data.T)\n",
        "        else:\n",
        "             # Manual calculation for older PyTorch or specific needs\n",
        "             cov_matrix = (block_data.T @ block_data) / (n_samples - 1)\n",
        "             # Ensure manual calc returns 2D even for 1 feature\n",
        "             if cov_matrix.ndim == 0:\n",
        "                 cov_matrix = cov_matrix.view(1,1) # Make it 1x1 matrix\n",
        "        cov_matrices.append(cov_matrix)\n",
        "\n",
        "    # Clamp small variances/diagonals to avoid numerical issues\n",
        "    variances = variances.clamp(min=1e-8)\n",
        "    for i in range(len(cov_matrices)): # Iterate using index to modify in place if needed\n",
        "        cov_matrix = cov_matrices[i]\n",
        "        if cov_matrix.numel() == 0: continue # Skip empty\n",
        "        # Check dimension before calling diag or clamping scalar\n",
        "        if cov_matrix.ndim == 2: # It's a 2D covariance matrix\n",
        "             diag = torch.diag(cov_matrix)\n",
        "             diag.clamp_(min=1e-8)\n",
        "             # Explicitly set the diagonal back into the matrix\n",
        "             cov_matrix.diagonal().copy_(diag)\n",
        "        elif cov_matrix.ndim == 0: # It's a 0D variance scalar\n",
        "             # Clamp the scalar value and update the list entry\n",
        "             cov_matrices[i] = cov_matrix.clamp(min=1e-8)\n",
        "        else:\n",
        "             # Should not happen if torch.cov or manual calc behaves as expected\n",
        "             print(f\"Warning: Unexpected tensor dimension {cov_matrix.ndim} in cov_matrices.\")\n",
        "    return variances, cov_matrices\n",
        "\n",
        "# Get object attribute helper (replace generic.get_object)\n",
        "def get_object(obj: Any, attributes: List[str]) -> Any:\n",
        "    \"\"\"Access nested attributes.\"\"\"\n",
        "    current = obj\n",
        "    for attr in attributes:\n",
        "        if isinstance(current, nn.ModuleDict) or isinstance(current, nn.ParameterDict):\n",
        "             current = current[attr]\n",
        "        elif isinstance(current, nn.ModuleList) or isinstance(current, nn.Sequential):\n",
        "            # Handle numerical indices for lists/sequential\n",
        "            try:\n",
        "                idx = int(attr)\n",
        "                current = current[idx]\n",
        "            except (ValueError, IndexError):\n",
        "                # Try accessing by attribute name if index fails (e.g., named modules in Sequential)\n",
        "                try:\n",
        "                    current = getattr(current, attr)\n",
        "                except AttributeError:\n",
        "                    raise AttributeError(f\"Cannot access attribute or index '{attr}' in {type(current)}\")\n",
        "        else:\n",
        "             current = getattr(current, attr)\n",
        "    return current\n",
        "\n",
        "# Pop with prefix helper (replace neural.pop_with_prefix)\n",
        "def pop_with_prefix(d: Dict[str, Any], prefix: str) -> Dict[str, Any]:\n",
        "    \"\"\"Remove items with a given prefix from dict and return them.\"\"\"\n",
        "    popped = {}\n",
        "    keys_to_remove = []\n",
        "    for key, value in d.items():\n",
        "        if key.startswith(prefix):\n",
        "            popped[key[len(prefix):]] = value\n",
        "            keys_to_remove.append(key)\n",
        "    for key in keys_to_remove:\n",
        "        d.pop(key)\n",
        "    return popped\n",
        "\n",
        "# --- NME Code (Author's Version with Functional Fix) ---\n",
        "# LinearMixedEffects class is not used by NeuralMixedEffects,\n",
        "# but kept here for completeness if needed later.\n",
        "class LinearMixedEffects(LossModule): # Note: Depends on neural.LossModule base\n",
        "    \"\"\"An activation function that learns a random effects. (Adapted)\"\"\"\n",
        "    truncate: Final[int]\n",
        "    only_bias: Final[bool]\n",
        "    reml: Final[bool]\n",
        "    device: Final[str]\n",
        "    add_bias: Final[bool]\n",
        "    log_2_pi: Final[float]\n",
        "    random_effect_ids: torch.Tensor\n",
        "    random_effects: torch.Tensor\n",
        "    sigma_2: torch.Tensor\n",
        "    d_sigma: torch.Tensor\n",
        "    dtype: torch.dtype\n",
        "\n",
        "    def __init__(\n",
        "        self, *, embedding_size: int = -1, output_size: int = 1, truncate: int = 4096,\n",
        "        add_bias: bool = True, only_bias: bool = False, number_of_cluster: int = -1,\n",
        "        dtype: torch.dtype = TENSOR_DTYPE, reml: bool = False, device: str = \"cpu\",\n",
        "        iterations: int = 0, **kwargs: Any,\n",
        "    ) -> None:\n",
        "        super().__init__(iterations=iterations, dtype=dtype, **kwargs)\n",
        "        if only_bias: add_bias = True; embedding_size = 0\n",
        "        self.truncate = truncate; self.only_bias = only_bias; self.reml = reml;\n",
        "        self.device = device; self.dtype = dtype\n",
        "        if embedding_size > 0: self.norm = torch.nn.LayerNorm(embedding_size, elementwise_affine=False, dtype=self.dtype)\n",
        "        else: self.norm = nn.Identity()\n",
        "        self.add_bias = add_bias\n",
        "        current_embedding_size = embedding_size\n",
        "        if self.add_bias: embedding_size = embedding_size + 1\n",
        "        if embedding_size <= 0: raise ValueError(\"Embedding size (including bias) must be positive.\")\n",
        "\n",
        "        self.register_buffer(\"d_sigma\", torch.eye(embedding_size, dtype=dtype).repeat(output_size, 1, 1))\n",
        "        self.register_buffer(\"sigma_2\", -torch.ones(output_size, dtype=dtype))\n",
        "        assert number_of_cluster > 0\n",
        "        self.register_buffer(\"random_effects\", torch.zeros(output_size, number_of_cluster, embedding_size, dtype=dtype))\n",
        "        self.register_buffer(\"random_effect_ids\", -torch.ones(number_of_cluster, dtype=torch.long))\n",
        "        self.log_2_pi = log(2 * pi)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_clusters(meta_id: torch.Tensor) -> dict[int, torch.Tensor]:\n",
        "        unique_ids, inverse_indices = torch.unique(meta_id, return_inverse=True)\n",
        "        clusters_by_id = defaultdict(list); [clusters_by_id[unique_ids[id_index].item()].append(i) for i, id_index in enumerate(inverse_indices)]\n",
        "        clusters_by_size = defaultdict(list); [clusters_by_size[len(indices)].append(torch.tensor(indices, dtype=torch.long, device=meta_id.device)) for id_val, indices in clusters_by_id.items()]\n",
        "        results: dict[int, torch.Tensor] = {}; [results.__setitem__(size, torch.stack(list_of_index_tensors, dim=0)) for size, list_of_index_tensors in clusters_by_size.items()]\n",
        "        return results\n",
        "\n",
        "    def estimate_random_effects(self, y: torch.Tensor, ids: torch.Tensor, random: torch.Tensor) -> None:\n",
        "        ids = ids.to(torch.long); buffer_device = self.sigma_2.device\n",
        "        random_processed = self._preprocess(random.to(device=buffer_device, dtype=self.dtype, non_blocking=True))\n",
        "        full_clusters: Optional[Dict[int, torch.Tensor]] = None\n",
        "        if self.truncate is not None and self.truncate > 0:\n",
        "            clusters = self._get_clusters(ids.to(buffer_device)); keep = torch.ones(y.shape[0], dtype=torch.bool, device=buffer_device); indices_to_drop = []\n",
        "            for observations, groups in clusters.items():\n",
        "                if observations <= self.truncate: continue\n",
        "                num_groups_of_this_size = groups.shape[0]\n",
        "                for i in range(num_groups_of_this_size):\n",
        "                    group_indices = groups[i]; perm = torch.randperm(observations, device=buffer_device); drop_in_group = perm[: observations - self.truncate]\n",
        "                    original_indices_to_drop = group_indices[drop_in_group]; indices_to_drop.extend(original_indices_to_drop.tolist())\n",
        "            if indices_to_drop:\n",
        "                keep[torch.tensor(indices_to_drop, device=buffer_device, dtype=torch.long)] = False\n",
        "                y = y.to(buffer_device)[keep]; ids = ids.to(buffer_device)[keep]; random_processed = random_processed.to(buffer_device)[keep]; full_clusters = None\n",
        "            else: full_clusters = clusters; y = y.to(buffer_device); ids = ids.to(buffer_device); random_processed = random_processed.to(buffer_device)\n",
        "        else: y = y.to(buffer_device); ids = ids.to(buffer_device); random_processed = random_processed.to(buffer_device); full_clusters = self._get_clusters(ids)\n",
        "        self._em(y.to(dtype=self.dtype, non_blocking=True), ids.to(dtype=torch.long, non_blocking=True), random_processed.to(dtype=self.dtype, non_blocking=True), full_clusters=full_clusters)\n",
        "\n",
        "    def _preprocess(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.to(dtype=self.dtype)\n",
        "        if not self.only_bias and x.shape[1] > 0 : x = self.norm(x)\n",
        "        if self.add_bias:\n",
        "            bias_term = torch.ones(x.shape[0], 1, device=x.device, dtype=self.dtype)\n",
        "            if x.shape[1] > 0: x = torch.cat([x, bias_term], dim=1)\n",
        "            else: x = bias_term\n",
        "        if self.only_bias: return x[:, -1:]\n",
        "        return x\n",
        "\n",
        "    def forward(self, y_fixed: torch.Tensor, meta: dict[str, torch.Tensor], y: Optional[torch.Tensor] = None, dataset: str = \"\") -> tuple[torch.Tensor, dict[str, torch.Tensor]]:\n",
        "        target_device = self.sigma_2.device; y_fixed = y_fixed.to(device=target_device, dtype=self.dtype)\n",
        "        meta = {k: v.to(device=target_device) if isinstance(v, torch.Tensor) else v for k, v in meta.items()}; meta['meta_id'] = meta['meta_id'].to(torch.long)\n",
        "\n",
        "        if 'meta_embedding' in meta and meta['meta_embedding'].shape[1] > 0: x = self._preprocess(meta[\"meta_embedding\"].to(target_device, dtype=self.dtype))\n",
        "        elif self.add_bias: x = torch.ones(y_fixed.shape[0], 1, device=target_device, dtype=self.dtype)\n",
        "        else: x = torch.empty(y_fixed.shape[0], 0, device=target_device, dtype=self.dtype)\n",
        "\n",
        "        batch_size = x.shape[0]; num_random_features = x.shape[1]; num_outputs = y_fixed.shape[1]\n",
        "        batch_random_effects = torch.zeros(batch_size, num_random_features, num_outputs, device=target_device, dtype=self.dtype)\n",
        "        self.random_effect_ids = self.random_effect_ids.to(target_device); self.random_effects = self.random_effects.to(target_device)\n",
        "\n",
        "        matches = torch.nonzero(self.random_effect_ids.unsqueeze(0) == meta[\"meta_id\"].unsqueeze(1), as_tuple=False)\n",
        "        if matches.numel() > 0:\n",
        "             for i in range(batch_size):\n",
        "                 current_id = meta['meta_id'][i].item(); match_idx = torch.where(self.random_effect_ids == current_id)[0]\n",
        "                 if match_idx.numel() > 0: known_cluster_idx = match_idx[0]; batch_random_effects[i] = self.random_effects[:, known_cluster_idx, :].permute(1, 0)\n",
        "\n",
        "        if num_random_features > 0: random_contribution = x.unsqueeze(1).bmm(batch_random_effects).squeeze(1); y_hats = y_fixed + random_contribution\n",
        "        else: y_hats = y_fixed\n",
        "\n",
        "        meta[\"meta_y_hat_fixed\"] = y_fixed; meta[\"meta_y_hat\"] = y_hats\n",
        "        return y_hats, meta\n",
        "\n",
        "    def add_new_random_effects(self, x: torch.Tensor, y_y_fixed: torch.Tensor, ids: torch.Tensor) -> None:\n",
        "        target_device = self.sigma_2.device; ids = ids.to(target_device, dtype=torch.long); x = x.to(target_device, dtype=self.dtype); y_y_fixed = y_y_fixed.to(target_device, dtype=self.dtype)\n",
        "        self.random_effect_ids = self.random_effect_ids.to(target_device); self.random_effects = self.random_effects.to(target_device)\n",
        "        is_existing = torch.isin(ids, self.random_effect_ids)\n",
        "        if is_existing.any():\n",
        "             keep = ~is_existing;\n",
        "             if not keep.any(): print(\"No new IDs to add.\"); return\n",
        "             x = x[keep]; y_y_fixed = y_y_fixed[keep]; ids = ids[keep]\n",
        "        if ids.numel() == 0: print(\"No new IDs to add after filtering.\"); return\n",
        "\n",
        "        print(f\"Adding effects for {len(torch.unique(ids))} new subjects.\")\n",
        "        saved_state = {\"d_sigma\": self.d_sigma.clone(), \"sigma_2\": self.sigma_2.clone(), \"random_effects\": self.random_effects.clone(), \"random_effect_ids\": self.random_effect_ids.clone()}\n",
        "        num_new_clusters = len(torch.unique(ids)); embedding_size = self.random_effects.shape[2]; output_size = self.random_effects.shape[0]\n",
        "\n",
        "        self.random_effects = torch.zeros(output_size, num_new_clusters, embedding_size, dtype=self.dtype, device=target_device)\n",
        "        self.random_effect_ids = -torch.ones(num_new_clusters, dtype=torch.long, device=target_device)\n",
        "        self.estimate_random_effects(y_y_fixed, ids, x)\n",
        "\n",
        "        estimated_new_effects = self.random_effects.clone(); estimated_new_ids = self.random_effect_ids.clone()\n",
        "        self.d_sigma = saved_state[\"d_sigma\"]; self.sigma_2 = saved_state[\"sigma_2\"]\n",
        "        valid_new_mask = estimated_new_ids != -1\n",
        "        if not valid_new_mask.any(): print(\"Warning: EM step did not yield valid new random effects.\"); self.random_effects = saved_state[\"random_effects\"]; self.random_effect_ids = saved_state[\"random_effect_ids\"]; return\n",
        "\n",
        "        self.random_effects = torch.cat([saved_state[\"random_effects\"], estimated_new_effects[:, valid_new_mask, :]], dim=1)\n",
        "        self.random_effect_ids = torch.cat([saved_state[\"random_effect_ids\"], estimated_new_ids[valid_new_mask]], dim=0)\n",
        "        print(f\"Total random effects stored: {self.random_effect_ids.shape[0]}\")\n",
        "\n",
        "    def _em(self, residual: torch.Tensor, ids: torch.Tensor, random: torch.Tensor, full_clusters: Optional[Dict[int, torch.Tensor]] = None) -> None:\n",
        "        device = self.sigma_2.device; lmes = residual.shape[1]; num_random_features = random.shape[1]\n",
        "        estimated_b_obs = torch.zeros(lmes, residual.shape[0], num_random_features, device=device, dtype=self.dtype)\n",
        "        self.sigma_2 = self.sigma_2.to(device); self.d_sigma = self.d_sigma.to(device)\n",
        "\n",
        "        if (self.sigma_2 == -1).all() or self.random_effects.abs().sum() == 0:\n",
        "            current_sigma_2 = residual.detach().pow(2).mean(dim=0).clamp(min=1e-5)\n",
        "            current_d_sigma = torch.eye(num_random_features, device=device, dtype=self.dtype).repeat(lmes, 1, 1) / current_sigma_2.view(-1, 1, 1).clamp(min=1e-5)\n",
        "            print(f\"Initialized sigma_2: {current_sigma_2.cpu().numpy()}, d_sigma diagonal: {current_d_sigma.diagonal(dim1=-2, dim2=-1).mean(dim=1).cpu().numpy()}\")\n",
        "            if self.sigma_2[0] == -1: self.sigma_2.copy_(current_sigma_2); self.d_sigma.copy_(current_d_sigma)\n",
        "        else: current_sigma_2 = self.sigma_2.clamp(min=1e-5); current_d_sigma = self.d_sigma\n",
        "\n",
        "        d_current = current_d_sigma * current_sigma_2.view(-1, 1, 1)\n",
        "        if full_clusters is None: full_clusters = self._get_clusters(ids)\n",
        "        sum_e_b_bt = torch.zeros_like(d_current); sum_e_residual_sq = torch.zeros_like(current_sigma_2); cluster_count: int = 0\n",
        "\n",
        "        for cluster_size, clusters in full_clusters.items():\n",
        "            num_groups = clusters.shape[0]; cluster_count += num_groups; flat_indices = clusters.view(-1).to(device)\n",
        "            current_residual = residual[flat_indices].view(num_groups, cluster_size, lmes)\n",
        "            current_random = random[flat_indices].view(num_groups, cluster_size, num_random_features)\n",
        "            for j in range(lmes):\n",
        "                Dj = d_current[j]; sigma_2j = current_sigma_2[j]\n",
        "                ZDj = current_random @ Dj; ZDjZT = ZDj @ current_random.transpose(-1, -2)\n",
        "                I = torch.eye(cluster_size, device=device, dtype=self.dtype).expand(num_groups, -1, -1); V = ZDjZT + sigma_2j * I\n",
        "                try:\n",
        "                    V_chol = torch.linalg.cholesky(V); residual_j = current_residual[:, :, j].unsqueeze(-1)\n",
        "                    temp = torch.cholesky_solve(residual_j, V_chol); ZT_temp = torch.matmul(current_random.transpose(-1,-2), temp); b_hat_j = Dj @ ZT_temp\n",
        "                    b_hat_j_repeated = b_hat_j.repeat_interleave(cluster_size, dim=0).view(-1, num_random_features); estimated_b_obs[j, flat_indices, :] = b_hat_j_repeated\n",
        "                    temp_var = torch.cholesky_solve(ZDj, V_chol); Var_b_j = Dj - torch.matmul(ZDj.transpose(-1,-2), temp_var); E_b_bt_j = Var_b_j + torch.matmul(b_hat_j, b_hat_j.transpose(-1,-2))\n",
        "                    sum_e_b_bt[j] += E_b_bt_j.sum(dim=0)\n",
        "                    term1 = (residual_j**2).sum(dim=1); yT_Z = torch.matmul(residual_j.transpose(-1,-2), current_random); term2 = -2 * torch.matmul(yT_Z, b_hat_j).squeeze(-1)\n",
        "                    ZT_Z = torch.matmul(current_random.transpose(-1,-2), current_random); term3 = torch.sum(ZT_Z * E_b_bt_j.transpose(-1,-2), dim=(-1,-2)).unsqueeze(-1)\n",
        "                    sum_e_residual_sq[j] += (term1 + term2 + term3).sum(dim=0)\n",
        "                except torch.linalg.LinAlgError as e: print(f\"Cholesky failed for LME {j}, cluster size {cluster_size}. Skipping. Error: {e}\"); continue\n",
        "\n",
        "        if cluster_count > 0:\n",
        "            new_D = sum_e_b_bt / cluster_count; new_D = (new_D + new_D.transpose(-1, -2)) / 2\n",
        "            total_observations = residual.shape[0]; new_sigma_2 = sum_e_residual_sq / total_observations; new_sigma_2.clamp_(min=1e-5)\n",
        "            self.sigma_2.copy_(new_sigma_2); self.d_sigma.copy_(new_D / new_sigma_2.view(-1, 1, 1).clamp(min=1e-5))\n",
        "\n",
        "            unique_ids_in_batch, index = unique_with_index(ids); num_unique_ids = len(unique_ids_in_batch)\n",
        "            _, inverse_indices = torch.unique(ids, return_inverse=True)\n",
        "            avg_effects = torch.zeros(lmes, num_unique_ids, num_random_features, device=device, dtype=self.dtype); id_counts = torch.zeros(num_unique_ids, device=device, dtype=torch.long)\n",
        "            id_counts.scatter_add_(0, inverse_indices, torch.ones_like(inverse_indices, dtype=torch.long))\n",
        "            for j in range(lmes): avg_effects[j].scatter_add_(0, inverse_indices.unsqueeze(-1).expand_as(estimated_b_obs[j]), estimated_b_obs[j])\n",
        "            valid_counts_mask = id_counts > 0\n",
        "            if valid_counts_mask.any(): avg_effects[:, valid_counts_mask, :] /= id_counts[valid_counts_mask].view(1, -1, 1).to(self.dtype)\n",
        "            self.random_effects = avg_effects.clone(); self.random_effect_ids = unique_ids_in_batch.clone()\n",
        "        else: print(\"Warning: EM step skipped due to zero clusters processed.\")\n",
        "\n",
        "    def _lme_d_sigma(*args, **kwargs): raise NotImplementedError(\"REML d_sigma adjustment not implemented.\")\n",
        "    def _lme_random_effects(*args, **kwargs): raise NotImplementedError(\"REML helper _lme_random_effects not implemented.\")\n",
        "    def _lme_x_vinv_x(*args, **kwargs): raise NotImplementedError(\"REML helper _lme_x_vinv_x not implemented.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# NME Class - Author's Version (Requires Ensemble helper) + Functional Forward\n",
        "# ============================================================================\n",
        "class NeuralMixedEffects(LossModule): # Note: Depends on neural.LossModule base\n",
        "    \"\"\"Fit non-linear mixed effect models.\"\"\"\n",
        "    simulated_annealing_alpha: Final[float]\n",
        "    dtype: Final[torch.dtype]\n",
        "    l2_lambda: Final[float]\n",
        "    p_eta: torch.Tensor\n",
        "    index_diagonal: Final[torch.Tensor]\n",
        "    index_full: Final[torch.Tensor]\n",
        "    models: nn.ModuleList # Uses N model instances\n",
        "    fixed_model: Optional[LossModule]\n",
        "    cluster_names: torch.Tensor\n",
        "    cluster_count: torch.Tensor # Original code uses this in loss\n",
        "    mixed_parameters: Dict[str, List[nn.Parameter]] # Only used for init now\n",
        "    random: nn.ParameterDict\n",
        "    fixed: nn.ParameterDict\n",
        "    sigma_diagonal: torch.Tensor\n",
        "    sigma_full: torch.Tensor\n",
        "    sigma_2: torch.Tensor\n",
        "    sa_sigma: torch.Tensor\n",
        "    during_training: bool\n",
        "    losses: list[torch.Tensor]\n",
        "    cluster_id_to_index: Dict[int, int]\n",
        "    random_effect_names: Tuple[str, ...]\n",
        "\n",
        "    # @beartype removed\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        clusters: torch.Tensor,\n",
        "        model_fun: type[LossModule] = MLP, # Note: Depends on neural.MLP\n",
        "        fixed_model_fun: type[LossModule] | None = None, # Note: Depends on neural.LossModule\n",
        "        random_effects: tuple[str, ...] = (),\n",
        "        random_buffers: tuple[str, ...] = (),\n",
        "        simulated_annealing_alpha: float = 0.97,\n",
        "        cluster_count: torch.Tensor, # Required by original loss fn\n",
        "        dtype: torch.dtype = TENSOR_DTYPE, # Use global dtype\n",
        "        independent: tuple[str, ...] = (),\n",
        "        l2_lambda: float = 1.0,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        \"\"\"Instantiate a non-linear mixed effects model. (Author's version structure)\"\"\"\n",
        "        # separate keywords for the mixed/fixed models\n",
        "        model_kwargs = pop_with_prefix(kwargs, \"model_\")\n",
        "        fixed_model_kwargs = pop_with_prefix(kwargs, \"fixed_model_\")\n",
        "\n",
        "        # Ensure output_size and input_size are correctly passed\n",
        "        output_size = kwargs.get(\"output_size\", model_kwargs.get(\"output_size\"))\n",
        "        input_size = kwargs.get(\"input_size\", fixed_model_kwargs.get(\"input_size\"))\n",
        "\n",
        "        if output_size is None: raise ValueError(\"output_size is required\")\n",
        "        if input_size is None and fixed_model_fun is None: raise ValueError(\"input_size is required if fixed_model_fun is None\")\n",
        "        if input_size is None and fixed_model_fun is not None: raise ValueError(\"input_size is required for fixed_model_fun\")\n",
        "\n",
        "\n",
        "        model_kwargs.setdefault(\"output_size\", output_size)\n",
        "        if fixed_model_fun is None:\n",
        "             model_kwargs.setdefault(\"input_size\", input_size)\n",
        "             fixed_model_kwargs.setdefault(\"input_size\", input_size) # For consistency\n",
        "        else:\n",
        "            fixed_model_kwargs.setdefault(\"input_size\", input_size)\n",
        "            fixed_size = fixed_model_kwargs.get(\"output_size\", model_kwargs.get(\"input_size\"))\n",
        "            if fixed_size is None:\n",
        "                 warnings.warn(\"Cannot infer fixed_model output size. Defaulting.\")\n",
        "                 fixed_size = input_size # Fallback\n",
        "            fixed_model_kwargs.setdefault(\"output_size\", fixed_size)\n",
        "            model_kwargs.setdefault(\"input_size\", fixed_size)\n",
        "\n",
        "\n",
        "        # replace \".\" with \"-\" for internal consistency in original code\n",
        "        random_effects_std = tuple(name.replace(\".\", \"-\") for name in random_effects)\n",
        "        independent_std = tuple(name.replace(\".\", \"-\") for name in independent)\n",
        "        self.random_effect_names = random_effects_std\n",
        "\n",
        "        super().__init__(**kwargs) # Pass remaining kwargs up\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.simulated_annealing_alpha = simulated_annealing_alpha\n",
        "        self.dtype = dtype\n",
        "        self.register_buffer(\"cluster_count\", cluster_count.view(-1).to(self.dtype)) # Ensure correct dtype\n",
        "        self.during_training = False\n",
        "        self.losses: list[torch.Tensor] = []\n",
        "\n",
        "        # instantiate purely fixed model\n",
        "        self.fixed_model = None\n",
        "        if fixed_model_fun is not None:\n",
        "            fixed_model_kwargs.setdefault('dtype', self.dtype) # Pass dtype\n",
        "            self.fixed_model = fixed_model_fun(**fixed_model_kwargs)\n",
        "            print(f\"Instantiated fixed model: {type(self.fixed_model)}\")\n",
        "\n",
        "        # instantiate N models (mixed models)\n",
        "        assert clusters.numel() == cluster_count.numel()\n",
        "        self.cluster_names = clusters.view(-1).to(torch.long) # Ensure long IDs\n",
        "        self.cluster_id_to_index = {cid.item(): idx for idx, cid in enumerate(self.cluster_names)}\n",
        "        num_clusters = len(self.cluster_names)\n",
        "        print(f\"Instantiating {num_clusters} models for NME...\")\n",
        "        model_kwargs.setdefault('dtype', self.dtype) # Pass dtype to model_fun\n",
        "\n",
        "        # *** Use nn.ModuleList directly, not Ensemble helper ***\n",
        "        self.models = nn.ModuleList([model_fun(**model_kwargs) for _ in range(num_clusters)])\n",
        "        print(f\"Instantiated NME models: {type(self.models[0])}\")\n",
        "\n",
        "\n",
        "        # keep reference of mixed parameters and share fixed parameters\n",
        "        self.mixed_parameters = defaultdict(list) # Still needed for initialization\n",
        "        shared_parameters = {}\n",
        "        shared_buffers = {}\n",
        "        first_model = self.models[0]\n",
        "        param_is_random = {}\n",
        "        buffer_is_random = {}\n",
        "\n",
        "        for name, param in first_model.named_parameters():\n",
        "            std_name = name.replace(\".\", \"-\")\n",
        "            is_random = std_name in self.random_effect_names\n",
        "            param_is_random[name] = is_random\n",
        "            if is_random:\n",
        "                self.mixed_parameters[std_name].append(param)\n",
        "            else:\n",
        "                shared_parameters[name] = param\n",
        "\n",
        "        for name, buf in first_model.named_buffers():\n",
        "             is_random_buffer = name in random_buffers\n",
        "             buffer_is_random[name] = is_random_buffer\n",
        "             if not is_random_buffer:\n",
        "                 shared_buffers[name] = buf\n",
        "\n",
        "        # Link shared parameters/buffers in other models\n",
        "        for i, model in enumerate(self.models[1:], 1):\n",
        "            for name, param in model.named_parameters():\n",
        "                if not param_is_random[name]:\n",
        "                    try:\n",
        "                        name_parts = name.split('.'); parent_module = get_object(model, name_parts[:-1]); attr_name = name_parts[-1]\n",
        "                        setattr(parent_module, attr_name, shared_parameters[name])\n",
        "                    except Exception as e: print(f\"Error linking shared param '{name}' in model {i}: {e}\")\n",
        "            for name, buf in model.named_buffers():\n",
        "                 if not buffer_is_random[name]:\n",
        "                     try:\n",
        "                         name_parts = name.split('.'); parent_module = get_object(model, name_parts[:-1]); attr_name = name_parts[-1]\n",
        "                         setattr(parent_module, attr_name, shared_buffers[name])\n",
        "                     except Exception as e: print(f\"Error linking shared buffer '{name}' in model {i}: {e}\")\n",
        "\n",
        "\n",
        "        # separate mixed parameters into fixed and random\n",
        "        self.random = nn.ParameterDict()\n",
        "        self.fixed = nn.ParameterDict()\n",
        "        if not self.mixed_parameters: print(\"Warning: No random effects specified.\")\n",
        "\n",
        "        for key, values in self.mixed_parameters.items():\n",
        "            if not values: continue\n",
        "            # Initialize fixed effect (beta) using the first model's parameter\n",
        "            self.fixed[key] = nn.Parameter(values[0].data.clone().to(self.dtype))\n",
        "            # Initialize random effects (eta) to zero for all clusters\n",
        "            self.random[key] = nn.Parameter(\n",
        "                torch.zeros((num_clusters, *values[0].shape), dtype=self.dtype)\n",
        "            )\n",
        "            print(f\"Tracking random effect '{key}' with shape: {self.random[key].shape}, dtype: {self.random[key].dtype}\")\n",
        "\n",
        "        # get size of random effects\n",
        "        number_random_effects = 0\n",
        "        parameter_index = {}\n",
        "        for name, parameter in self.fixed.items(): # Use self.fixed for reference shape/size\n",
        "            size = parameter.numel()\n",
        "            parameter_index[name] = list(\n",
        "                range(number_random_effects, number_random_effects + size),\n",
        "            )\n",
        "            number_random_effects += size\n",
        "        print(f\"Total scalar random effects per cluster: {number_random_effects}\")\n",
        "\n",
        "\n",
        "        # inverting covariance structure (full or independent)\n",
        "        full = tuple(sorted(set(self.mixed_parameters.keys()).difference(independent_std)))\n",
        "        independent = tuple(\n",
        "            set(independent_std).intersection(self.mixed_parameters.keys()),\n",
        "        )\n",
        "\n",
        "        independent_indices_list = [parameter_index[x] for x in independent] if independent else []\n",
        "        independent_indices = torch.tensor(generic_flatten_nested_list(independent_indices_list), dtype=torch.long)\n",
        "        full_indices_list = [parameter_index[x] for x in full] if full else []\n",
        "        full_indices = torch.tensor(generic_flatten_nested_list(full_indices_list), dtype=torch.long)\n",
        "\n",
        "        print(f\"Independent effect indices: {independent_indices.tolist()}\")\n",
        "        print(f\"Full covariance block indices: {full_indices.tolist()}\")\n",
        "\n",
        "        self.register_buffer(\"sigma_diagonal\", torch.ones(len(independent_indices), dtype=self.dtype))\n",
        "        self.register_buffer(\"sigma_full\", torch.eye(len(full_indices), dtype=self.dtype))\n",
        "        self.index_diagonal = independent_indices\n",
        "        self.index_full = full_indices\n",
        "\n",
        "        # variables for 'SAEM' (for M)\n",
        "        self.register_buffer(\"sigma_2\", torch.tensor([0.1], dtype=self.dtype)) # Initial guess 0.1\n",
        "        self.register_buffer(\"sa_sigma\", torch.ones(number_random_effects, dtype=self.dtype))\n",
        "        self.register_buffer(\"p_eta\", torch.zeros(len(self.models), dtype=self.dtype)) # Match number of models\n",
        "\n",
        "    def named_parameters(\n",
        "        self,\n",
        "        prefix: str = \"\",\n",
        "        recurse: bool = True,\n",
        "    ) -> Generator[tuple[str, torch.nn.Parameter], None, None]:\n",
        "        \"\"\"Generator over parameters optimized by NME.\"\"\"\n",
        "        param_names_yielded = set()\n",
        "\n",
        "        # Yield parameters from the fixed_model if it exists\n",
        "        if self.fixed_model is not None:\n",
        "            for name, param in self.fixed_model.named_parameters(prefix=prefix + \"fixed_model\", recurse=recurse):\n",
        "                 full_name = prefix + \"fixed_model.\" + name # Original name includes \"fixed_model.\" prefix\n",
        "                 if full_name not in param_names_yielded:\n",
        "                     yield full_name, param\n",
        "                     param_names_yielded.add(full_name)\n",
        "\n",
        "\n",
        "        # Yield the 'fixed' part of the random effects (mean effect, beta)\n",
        "        for name, param in self.fixed.named_parameters(prefix=prefix + \"fixed\", recurse=recurse):\n",
        "             full_name = prefix + \"fixed.\" + name\n",
        "             if full_name not in param_names_yielded:\n",
        "                 yield full_name, param\n",
        "                 param_names_yielded.add(full_name)\n",
        "\n",
        "        # Yield the 'random' part of the random effects (deviations, eta)\n",
        "        for name, param in self.random.named_parameters(prefix=prefix + \"random\", recurse=recurse):\n",
        "             full_name = prefix + \"random.\" + name\n",
        "             if full_name not in param_names_yielded:\n",
        "                 yield full_name, param\n",
        "                 param_names_yielded.add(full_name)\n",
        "\n",
        "        # Yield the shared parameters from the *first* model instance\n",
        "        if self.models:\n",
        "            first_model = self.models[0]\n",
        "            for name, param in first_model.named_parameters(recurse=recurse):\n",
        "                std_name = name.replace(\".\", \"-\")\n",
        "                # Yield only if it's NOT a random effect parameter\n",
        "                if std_name not in self.random_effect_names:\n",
        "                    full_name = prefix + \"models.shared.\" + name # Adjust prefix for clarity\n",
        "                    if full_name not in param_names_yielded:\n",
        "                        yield full_name, param\n",
        "                        param_names_yielded.add(full_name)\n",
        "\n",
        "    def can_jit(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def _update_sigma_2_sigma(self) -> None:\n",
        "        \"\"\"Update random effect covariance (Sigma) and residual variance (sigma_2).\"\"\"\n",
        "        target_device = self.sigma_2.device # Use a buffer's device\n",
        "\n",
        "        # --- Update sigma_2 (Residual Variance) ---\n",
        "        if self.losses:\n",
        "             new_sigma_2 = torch.stack(self.losses).mean().clamp(min=1e-8)\n",
        "             self.sigma_2.copy_(new_sigma_2.to(target_device, dtype=self.dtype))\n",
        "        self.losses = [] # Clear accumulated losses\n",
        "\n",
        "        # --- Update Sigma (Random Effect Covariance) ---\n",
        "        if not self.random: # Skip if no random effects defined\n",
        "             return\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eta = self._get_eta() # Shape: (num_clusters, num_random_effects), dtype=self.dtype, device=target_device\n",
        "            if eta.shape[0] <= 1: return\n",
        "\n",
        "            idx_diag = self.index_diagonal.to(target_device)\n",
        "            idx_full = self.index_full.to(target_device)\n",
        "\n",
        "            variances, sigmas = cov_block(eta, idx_diag, [idx_full] if idx_full.numel() > 0 else [])\n",
        "\n",
        "            self.sa_sigma = self.sa_sigma.to(target_device)\n",
        "            self.sigma_diagonal = self.sigma_diagonal.to(target_device)\n",
        "            self.sigma_full = self.sigma_full.to(target_device)\n",
        "\n",
        "\n",
        "            if idx_diag.numel() > 0:\n",
        "                current_sa_diag = self.sa_sigma[idx_diag]\n",
        "                new_sa_diag = torch.max(current_sa_diag * self.simulated_annealing_alpha, variances)\n",
        "                self.sa_sigma[idx_diag] = new_sa_diag\n",
        "                self.sigma_diagonal.copy_(1.0 / new_sa_diag.clamp(min=1e-8))\n",
        "\n",
        "            if idx_full.numel() > 0 and sigmas:\n",
        "                if sigmas[0] is not None and sigmas[0].numel() > 0:\n",
        "                    sigma_empirical = sigmas[0]\n",
        "                    current_sa_full_diag = self.sa_sigma[idx_full]\n",
        "\n",
        "                    if sigma_empirical.ndim == 0: # Scalar variance case\n",
        "                         new_sa_diag_full = torch.max(current_sa_full_diag * self.simulated_annealing_alpha, sigma_empirical)\n",
        "                         self.sa_sigma[idx_full] = new_sa_diag_full\n",
        "                         if self.sigma_full.shape == (1,1):\n",
        "                             inv_sqrt_var = (1.0 / new_sa_diag_full.clamp(min=1e-8)).sqrt()\n",
        "                             self.sigma_full.fill_(inv_sqrt_var.item())\n",
        "                         else: print(f\"Warning: Shape mismatch for sigma_full ({self.sigma_full.shape}) in scalar case.\")\n",
        "\n",
        "                    elif sigma_empirical.ndim == 2: # Matrix covariance case\n",
        "                        new_sa_diag_full = torch.max(current_sa_full_diag * self.simulated_annealing_alpha, sigma_empirical.diagonal())\n",
        "                        self.sa_sigma[idx_full] = new_sa_diag_full\n",
        "\n",
        "                        sigma_annealed = sigma_empirical.clone()\n",
        "                        sigma_annealed.diagonal().copy_(new_sa_diag_full)\n",
        "\n",
        "                        try:\n",
        "                            identity = torch.eye(sigma_annealed.shape[0], device=target_device, dtype=self.dtype)\n",
        "                            chol_annealed = torch.linalg.cholesky(sigma_annealed)\n",
        "                            sigma_inv = torch.cholesky_solve(identity, chol_annealed)\n",
        "\n",
        "                            min_eig = torch.min(torch.linalg.eigvalsh(sigma_inv))\n",
        "                            diag_mean = torch.mean(torch.abs(sigma_inv.diagonal()))\n",
        "                            jitter_threshold = max(1e-8, diag_mean * 1e-6)\n",
        "\n",
        "                            if min_eig < jitter_threshold:\n",
        "                                jitter = jitter_threshold - min_eig\n",
        "                                sigma_inv.diagonal().add_(jitter)\n",
        "\n",
        "                            chol_sigma_inv = torch.linalg.cholesky(sigma_inv)\n",
        "                            chol_sigma_inv.diagonal().clamp_(min=0.0) # Ensure non-negative diagonal for Cholesky of inverse\n",
        "                            self.sigma_full.copy_(chol_sigma_inv)\n",
        "                        except torch.linalg.LinAlgError as e: print(f\"Warning: Cholesky failed during Sigma update: {e}. Sigma_full not updated.\")\n",
        "\n",
        "    def _get_eta(self) -> torch.Tensor:\n",
        "        target_device = self.sigma_2.device\n",
        "        if not self.random: return torch.empty((len(self.cluster_names), 0), device=target_device, dtype=self.dtype)\n",
        "\n",
        "        self.random.to(target_device) # Ensure ParameterDict's parameters are on target device\n",
        "        etas = [self.random[name].view(self.random[name].shape[0], -1).to(self.dtype) for name in self.random.keys()]\n",
        "        return torch.cat(etas, dim=1)\n",
        "\n",
        "\n",
        "    def _after_training(self) -> None:\n",
        "        if not (not self.training and self.during_training): return\n",
        "        self.during_training = False\n",
        "        with torch.no_grad(): self._update_sigma_2_sigma()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        if not mode: self._after_training()\n",
        "        super().train(mode)\n",
        "        if mode: self._after_training() # Should be if mode and not self.during_training ?\n",
        "                                       # Original code has self.during_training = True at start of train. Let's ensure that happens.\n",
        "                                       # The train() logic calls super().train(mode) first.\n",
        "                                       # Then, if mode is True, it calls _after_training(). This is to ensure _update_sigma runs at start of training.\n",
        "                                       # And if mode is False (eval), it calls _after_training() to run at end of training / start of eval.\n",
        "\n",
        "        # This structure seems to be:\n",
        "        # train(True) -> super().train(True) -> self.during_training=True -> _after_training() [updates based on previous epoch or init state]\n",
        "        # train(False) -> _after_training() [updates based on last training data] -> super().train(False) -> self.during_training=False\n",
        "        # Let's simplify based on common patterns: update sigma at the end of an epoch (when switching to eval or before next train).\n",
        "        # The current hook for _update_sigma_2_sigma in train()/eval() is a bit convoluted.\n",
        "        # A common way is:\n",
        "        # my_model.train() # sets self.training = True\n",
        "        # ... epoch loop ...\n",
        "        # my_model.eval() # sets self.training = False, triggers sigma update based on accumulated losses during training.\n",
        "        # Let's adjust based on author's intent: _after_training() seems to be the main hook.\n",
        "        # If mode is True (switching to train mode): set during_training = True. Update sigma if it was previously False.\n",
        "        # If mode is False (switching to eval mode): update sigma based on accumulated. Set during_training = False.\n",
        "\n",
        "        if mode: # Entering training mode\n",
        "            if not self.during_training: # If previously in eval mode or first time\n",
        "                 self._update_sigma_2_sigma() # Update based on whatever was before (e.g. init, or last eval)\n",
        "            self.during_training = True\n",
        "        else: # Entering eval mode\n",
        "            if self.during_training: # If previously in training mode\n",
        "                self._update_sigma_2_sigma() # This is the crucial update based on the training epoch's losses\n",
        "            self.during_training = False\n",
        "        super().train(mode) # This sets self.training\n",
        "        return self\n",
        "\n",
        "\n",
        "    def eval(self): return self.train(False)\n",
        "\n",
        "\n",
        "    # *** FORWARD PASS USING FUNCTIONAL RANDOM EFFECTS ***\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor, # Input features\n",
        "        meta: dict[str, torch.Tensor], # Metadata including 'meta_id'\n",
        "        y: Optional[torch.Tensor] = None, # Ground truth (optional)\n",
        "        dataset: str = \"\", # Optional dataset tag (unused)\n",
        "    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:\n",
        "        \"\"\"Forward pass using N model instances + functional effects.\"\"\"\n",
        "        target_device = self.sigma_2.device\n",
        "        x = x.to(target_device, dtype=self.dtype)\n",
        "        meta = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in meta.items()}\n",
        "        batch_ids = meta['meta_id'].to(torch.long)\n",
        "\n",
        "        if y is not None:\n",
        "             y = y.to(target_device, dtype=self.dtype)\n",
        "\n",
        "        # Ensure internal buffers/params are on the correct device\n",
        "        self.cluster_names = self.cluster_names.to(target_device)\n",
        "        self.fixed.to(target_device)\n",
        "        self.random.to(target_device)\n",
        "        self.models.to(target_device) # Move all sub-models\n",
        "        self.models.train(self.training) # Set sub-models mode\n",
        "\n",
        "        # --- Fixed Model Pass (if applicable) ---\n",
        "        if self.fixed_model is not None:\n",
        "            self.fixed_model = self.fixed_model.to(target_device)\n",
        "            self.fixed_model.train(self.training)\n",
        "            x_fixed, meta = self.fixed_model(x, meta, y=y, dataset=dataset)\n",
        "            x_mixed_input = x_fixed.to(self.dtype)\n",
        "        else:\n",
        "            x_mixed_input = x\n",
        "\n",
        "        # --- Map samples to models and process ---\n",
        "        # Create a mapping from batch_id to model_idx (-1 if unknown)\n",
        "        batch_indices = torch.tensor([self.cluster_id_to_index.get(bid.item(), -1) for bid in batch_ids], dtype=torch.long, device=target_device)\n",
        "\n",
        "        batch_size = x_mixed_input.shape[0]\n",
        "        # Infer output size from first model's final layer\n",
        "        try:\n",
        "            # *** Access network attribute correctly ***\n",
        "            output_size = self.models[0].network[-1].out_features\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not infer output size ({e}), defaulting to 1\")\n",
        "            output_size = 1\n",
        "\n",
        "        final_scores = torch.zeros(batch_size, output_size, device=target_device, dtype=self.dtype)\n",
        "\n",
        "        # Process known clusters (apply random effects functionally)\n",
        "        valid_mask = batch_indices != -1\n",
        "        if valid_mask.any():\n",
        "            valid_input = x_mixed_input[valid_mask]\n",
        "            valid_cluster_indices_for_batch = batch_indices[valid_mask] # These are model indices\n",
        "\n",
        "            # Iterate over unique model indices present in this batch\n",
        "            unique_model_indices_in_batch = torch.unique(valid_cluster_indices_for_batch)\n",
        "\n",
        "            temp_scores = torch.zeros(valid_input.shape[0], output_size, device=target_device, dtype=self.dtype)\n",
        "            temp_scores_original_indices = torch.arange(len(valid_input), device=target_device)\n",
        "\n",
        "\n",
        "            for model_idx_tensor in unique_model_indices_in_batch:\n",
        "                model_idx = model_idx_tensor.item() # Get scalar index\n",
        "\n",
        "                # Get all samples in the valid_input that belong to this model_idx\n",
        "                samples_for_this_model_mask = (valid_cluster_indices_for_batch == model_idx)\n",
        "                model_input_for_this_cluster = valid_input[samples_for_this_model_mask]\n",
        "\n",
        "                if model_input_for_this_cluster.shape[0] == 0: continue\n",
        "\n",
        "                model_instance = self.models[model_idx] # Get the specific model instance (for its structure)\n",
        "                                                    # but we'll use shared params + random effects\n",
        "\n",
        "                # Pass through model body (up to penultimate layer)\n",
        "                # *** Access network attribute correctly ***\n",
        "                if isinstance(model_instance, MLP):\n",
        "                    # Use the network of the *first model* for shared layers\n",
        "                    body_output = self.models[0].network[:-1](model_input_for_this_cluster)\n",
        "                    final_layer_template = self.models[0].network[-1] # Template for structure (e.g. bias existence)\n",
        "                else:\n",
        "                    raise TypeError(\"Functional forward requires MLP with accessible layers\")\n",
        "\n",
        "                # Apply final layer functionally with random effects\n",
        "                scores_cluster_for_model = body_output @ final_layer_template.weight.t() # Apply shared weight (beta_weight)\n",
        "\n",
        "                # Apply bias (fixed + random) if it exists and is specified as a random effect\n",
        "                # Assuming the last layer's bias is 'network.(N-1).bias' where N-1 is the index of the last Linear layer.\n",
        "                # If MLP has [H1, H2], layers are Lin(0), Act(1), Lin(2), Act(3), Lin(4) -> last bias is network.4.bias\n",
        "                # This needs to be robust to MLP structure. For now, assume fixed naming from example.\n",
        "                final_layer_idx_in_mlp_seq = -1 # The final Linear layer\n",
        "                bias_param_name_in_model = f\"network.{len(self.models[0].network) + final_layer_idx_in_mlp_seq}.bias\" # e.g. network.4.bias\n",
        "                bias_std_name = bias_param_name_in_model.replace(\".\", \"-\")\n",
        "\n",
        "\n",
        "                if bias_std_name in self.random_effect_names:\n",
        "                    fixed_bias = self.fixed[bias_std_name]       # beta_bias\n",
        "                    random_bias_all_clusters = self.random[bias_std_name] # eta_bias for all clusters\n",
        "                    eta_bias_for_this_cluster = random_bias_all_clusters[model_idx]\n",
        "                    combined_bias_for_cluster = fixed_bias + eta_bias_for_this_cluster\n",
        "                    scores_cluster_for_model = scores_cluster_for_model + combined_bias_for_cluster\n",
        "                elif final_layer_template.bias is not None: # Apply non-random shared bias if it exists\n",
        "                    scores_cluster_for_model = scores_cluster_for_model + final_layer_template.bias\n",
        "\n",
        "                # Apply other random effects (e.g., on weights) if configured - more complex\n",
        "                # Example for random effect on final layer's weight (if 'network.4.weight' was random)\n",
        "                # weight_param_name_in_model = f\"network.{len(self.models[0].network) + final_layer_idx_in_mlp_seq}.weight\"\n",
        "                # weight_std_name = weight_param_name_in_model.replace(\".\", \"-\")\n",
        "                # if weight_std_name in self.random_effect_names:\n",
        "                #     fixed_weight = self.fixed[weight_std_name] # beta_W\n",
        "                #     random_weight_all_clusters = self.random[weight_std_name] # eta_W\n",
        "                #     eta_weight_for_this_cluster = random_weight_all_clusters[model_idx]\n",
        "                #     combined_weight_for_cluster = fixed_weight + eta_weight_for_this_cluster\n",
        "                #     # Recompute scores_cluster with combined_weight_for_cluster\n",
        "                #     # scores_cluster_for_model = body_output @ combined_weight_for_cluster.t()\n",
        "                #     # ... then add bias as above.\n",
        "                #     # For now, this part is commented as only bias is random in example.\n",
        "                # else: # Already used shared weight: final_layer_template.weight\n",
        "\n",
        "                # Place scores back in the correct positions for the batch\n",
        "                indices_in_temp_scores = temp_scores_original_indices[samples_for_this_model_mask]\n",
        "                temp_scores[indices_in_temp_scores] = scores_cluster_for_model\n",
        "\n",
        "            # Assign the computed scores for valid samples back to the final output\n",
        "            final_scores[valid_mask] = temp_scores\n",
        "\n",
        "\n",
        "        # Handle unknown clusters (use average prediction? or fixed effects only?)\n",
        "        # Using fixed effects only (beta) from self.fixed\n",
        "        if not valid_mask.all(): # If there are any unknown clusters\n",
        "            unknown_mask = ~valid_mask\n",
        "            unknown_input = x_mixed_input[unknown_mask]\n",
        "            if unknown_input.shape[0] > 0:\n",
        "                # Use the first model instance as representative for shared params and structure\n",
        "                model0 = self.models[0]\n",
        "                if isinstance(model0, MLP):\n",
        "                    # *** Access network attribute correctly ***\n",
        "                    unknown_body_output = model0.network[:-1](unknown_input)\n",
        "                    final_layer = model0.network[-1]\n",
        "                else:\n",
        "                    raise TypeError(\"Functional forward for unknown clusters requires MLP\")\n",
        "\n",
        "                # Use the shared weight (beta_weight from fixed effects perspective)\n",
        "                # If final layer weight itself were random, fixed_weight = self.fixed['network.4.weight']\n",
        "                # Otherwise, it's just the shared parameter:\n",
        "                fixed_weight = final_layer.weight\n",
        "\n",
        "                # Determine bias: use beta (from self.fixed) if param is random, else use model param's bias\n",
        "                final_layer_idx_in_mlp_seq = -1\n",
        "                bias_param_name_in_model = f\"network.{len(self.models[0].network) + final_layer_idx_in_mlp_seq}.bias\"\n",
        "                bias_std_name = bias_param_name_in_model.replace(\".\", \"-\")\n",
        "\n",
        "                scores_unknown = torch.matmul(unknown_body_output, fixed_weight.t())\n",
        "                if bias_std_name in self.random_effect_names:\n",
        "                    fixed_bias_val = self.fixed[bias_std_name] # Use beta_bias\n",
        "                    scores_unknown = scores_unknown + fixed_bias_val\n",
        "                elif final_layer.bias is not None: # Use original shared non-random bias\n",
        "                    scores_unknown = scores_unknown + final_layer.bias\n",
        "                # Else: no bias to add\n",
        "\n",
        "                final_scores[unknown_mask] = scores_unknown\n",
        "\n",
        "        return final_scores, meta\n",
        "\n",
        "\n",
        "    def _get_p_eta(self, *, index: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Calculate 0.5 * eta^T Sigma^-1 eta for selected clusters.\"\"\"\n",
        "        target_device = self.sigma_2.device\n",
        "        if not self.random or index.numel() == 0:\n",
        "            return torch.zeros(index.numel(), device=target_device, dtype=self.dtype)\n",
        "\n",
        "        # Get eta values for the specified clusters (already correct dtype/device)\n",
        "        # Ensure index is valid (these are model indices, should be < num_clusters)\n",
        "        if index.max() >= len(self.cluster_names):\n",
        "             print(f\"Warning: Invalid index in _get_p_eta. Max index: {index.max()}, Num clusters: {len(self.cluster_names)}\")\n",
        "             valid_mask_idx = index < len(self.cluster_names); index = index[valid_mask_idx]\n",
        "             if index.numel() == 0: return torch.zeros(0, device=target_device, dtype=self.dtype)\n",
        "\n",
        "        eta_all_clusters = self._get_eta() # Shape: (num_total_clusters, num_random_effects)\n",
        "        eta = eta_all_clusters[index] # Shape: (len(index), num_random_effects)\n",
        "\n",
        "        # Original code subtracts mean, let's replicate that.\n",
        "        eta = eta - eta.mean(dim=0, keepdims=True) # Centering eta for the batch being processed\n",
        "\n",
        "        p_eta_val = torch.zeros(eta.shape[0], device=target_device, dtype=self.dtype)\n",
        "\n",
        "        # Ensure Sigma components are on the correct device\n",
        "        idx_diag = self.index_diagonal.to(target_device)\n",
        "        idx_full = self.index_full.to(target_device)\n",
        "        sigma_diag_inv_sqrt = self.sigma_diagonal.to(target_device) # This is 1/variance\n",
        "        sigma_full_L_inv = self.sigma_full.to(target_device) # Lower Cholesky factor of Sigma_inv\n",
        "\n",
        "        # --- Contribution from Independent Effects ---\n",
        "        if idx_diag.numel() > 0:\n",
        "            eta_diag = eta[:, idx_diag] # Shape: (len(index), num_independent)\n",
        "            # sigma_diagonal stores 1/variance. So (eta^2 / variance)\n",
        "            p_eta_diag_contrib = (eta_diag.pow(2) * sigma_diag_inv_sqrt.unsqueeze(0)).sum(dim=1)\n",
        "            p_eta_val += p_eta_diag_contrib\n",
        "\n",
        "        # --- Contribution from Full Covariance Block ---\n",
        "        if idx_full.numel() > 0:\n",
        "            eta_full = eta[:, idx_full] # Shape: (len(index), num_full)\n",
        "            L_inv = sigma_full_L_inv # This is L_inv such that Sigma_inv = L_inv L_inv^T\n",
        "\n",
        "            if L_inv.numel() > 0: # Check if L_inv is not empty\n",
        "                # We want eta^T Sigma_inv eta = eta^T L_inv L_inv^T eta = (L_inv^T eta)^T (L_inv^T eta)\n",
        "                # So, calculate v = L_inv^T eta, then sum squares of v.\n",
        "                if L_inv.shape == (1,1) and eta_full.shape[1] == 1: # scalar random effect in full block\n",
        "                    L_inv_T_eta = L_inv.T * eta_full # element-wise for (N_batch, 1)\n",
        "                elif L_inv.ndim == 2 and eta_full.ndim == 2:\n",
        "                    try:\n",
        "                        # L_inv is (k,k), eta_full is (N_batch, k)\n",
        "                        # L_inv.T is (k,k). eta_full.unsqueeze(-1) is (N_batch, k, 1)\n",
        "                        # bmm: (N_batch, k, k) @ (N_batch, k, 1) -> (N_batch, k, 1)\n",
        "                        # or (k,k) @ (k, N_batch) -> (k, N_batch) then transpose and sum\n",
        "                        L_inv_T_expanded = L_inv.T.unsqueeze(0).expand(eta_full.shape[0], -1, -1) # (N_batch, k, k)\n",
        "                        eta_full_vec = eta_full.unsqueeze(-1) # (N_batch, k, 1)\n",
        "                        L_inv_T_eta = torch.bmm(L_inv_T_expanded, eta_full_vec) # (N_batch, k, 1)\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"Error during p_eta calculation (bmm): {e}\")\n",
        "                        print(f\"Shapes: L_inv.T={L_inv.T.shape}, eta_full_vec shape={eta_full_vec.shape if 'eta_full_vec' in locals() else 'eta_full_vec not defined'}\")\n",
        "                        L_inv_T_eta = torch.zeros_like(eta_full.unsqueeze(-1)) # Fallback\n",
        "                else: # Should not happen with correct setup\n",
        "                    print(f\"Warning: Unexpected shapes for L_inv ({L_inv.shape}) or eta_full ({eta_full.shape})\")\n",
        "                    L_inv_T_eta = torch.zeros_like(eta_full.unsqueeze(-1))\n",
        "\n",
        "                p_eta_full_contrib = torch.sum(L_inv_T_eta.pow(2), dim=(1, 2) if L_inv_T_eta.ndim == 3 else 1)\n",
        "                p_eta_val += p_eta_full_contrib.clamp(min=0.0)\n",
        "            else: # L_inv is empty tensor (e.g. idx_full has elements but sigma_full is 0x0)\n",
        "                 pass # No contribution\n",
        "\n",
        "\n",
        "        # Return 0.5 * eta^T Sigma^-1 eta\n",
        "        return p_eta_val / 2.0\n",
        "\n",
        "\n",
        "    # @torch.jit.export # JIT export removed\n",
        "    def loss(\n",
        "        self,\n",
        "        scores: torch.Tensor,\n",
        "        ground_truth: torch.Tensor,\n",
        "        meta: dict[str, torch.Tensor],\n",
        "        take_mean: bool = True, # Original code didn't use this arg\n",
        "        loss: Optional[torch.Tensor] = None, # Original code didn't use this arg\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Calculate the NME loss using original author's logic structure but functional forward.\"\"\"\n",
        "        target_device = scores.device\n",
        "        ground_truth = ground_truth.to(target_device)\n",
        "        meta = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in meta.items()}\n",
        "\n",
        "        if 'meta_id' not in meta: raise KeyError(\"'meta_id' missing from meta\")\n",
        "        batch_ids = meta['meta_id'].to(torch.long)\n",
        "\n",
        "        self.cluster_names = self.cluster_names.to(target_device)\n",
        "        self.p_eta = self.p_eta.to(target_device) # This buffer seems to store per-cluster quadratic form from previous step\n",
        "        self.sigma_2 = self.sigma_2.to(target_device)\n",
        "        self.cluster_count = self.cluster_count.to(target_device) # Ensure on device\n",
        "\n",
        "        # p(y | eta, Theta) - Calculated using base loss on final scores\n",
        "        # base_loss returns sum of squared errors per sample if take_mean=False\n",
        "        loss_per_sample = super().loss(scores, ground_truth, {}, take_mean=False) # (batch_size,)\n",
        "        sum_sq_errors_batch = loss_per_sample.sum() # Sum of squared errors for the batch\n",
        "\n",
        "        # Store loss for sigma_2 update\n",
        "        if self.training:\n",
        "            # Store average squared error for sigma_2 update\n",
        "            avg_sq_error_batch = sum_sq_errors_batch / scores.shape[0] if scores.shape[0] > 0 else torch.tensor(0.0, device=target_device, dtype=self.dtype)\n",
        "            self.losses.append(avg_sq_error_batch.detach())\n",
        "\n",
        "        # Likelihood term: sum_i (y_i - f(x_i, eta_c(i)))^2 / (N_batch * sigma_e^2)\n",
        "        # This is (1/N_batch) * sum( (y_i - y_hat_i)^2 / sigma_e^2 )\n",
        "        # Original code: likelihood_term = loss_likelihood / scores.shape[0] / self.sigma_2.clamp(min=1e-8)\n",
        "        # Where loss_likelihood is sum_sq_errors_batch\n",
        "        # So it is: (sum_sq_errors_batch / N_batch) / sigma_e^2  = MSE_batch / sigma_e^2\n",
        "        likelihood_term = (sum_sq_errors_batch / scores.shape[0] if scores.shape[0] > 0 else 0.0) / self.sigma_2.clamp(min=1e-8)\n",
        "\n",
        "\n",
        "        # p(eta | Theta) - Regularization term: sum_clusters_in_batch (0.5 * eta_c^T Sigma_eta^-1 eta_c) / N_batch_clusters * lambda\n",
        "        # Map batch_ids to their model indices\n",
        "        batch_model_indices = torch.tensor([self.cluster_id_to_index.get(bid.item(), -1) for bid in batch_ids], dtype=torch.long, device=target_device)\n",
        "        valid_indices_mask = batch_model_indices != -1\n",
        "        unique_model_indices_in_batch = torch.unique(batch_model_indices[valid_indices_mask])\n",
        "\n",
        "        prior_term = torch.tensor(0.0, device=target_device, dtype=self.dtype)\n",
        "        if unique_model_indices_in_batch.numel() > 0: # If there are known clusters in the batch\n",
        "            # Calculate 0.5 * eta_c^T Sigma_eta^-1 eta_c for each unique cluster c in the batch\n",
        "            p_eta_for_unique_clusters_in_batch = self._get_p_eta(index=unique_model_indices_in_batch) # (num_unique_clusters_in_batch,)\n",
        "\n",
        "            # The original code's prior term seems to be an average of p_eta over samples belonging to known clusters\n",
        "            # model_idx_to_p_eta_value = {idx.item(): p_val for idx, p_val in zip(unique_model_indices_in_batch, p_eta_for_unique_clusters_in_batch)}\n",
        "            # sample_p_eta_values = torch.zeros(scores.shape[0], device=target_device, dtype=self.dtype)\n",
        "            # for i in range(scores.shape[0]):\n",
        "            #     model_idx = batch_model_indices[i].item()\n",
        "            #     if model_idx != -1:\n",
        "            #         sample_p_eta_values[i] = model_idx_to_p_eta_value[model_idx]\n",
        "            # avg_p_eta_for_batch = sample_p_eta_values[valid_indices_mask].mean() if valid_indices_mask.any() else torch.tensor(0.0)\n",
        "            # prior_term = avg_p_eta_for_batch * self.l2_lambda\n",
        "\n",
        "            # Simpler: average of p_eta values for unique clusters present in batch\n",
        "            # This matches the interpretation \"sum over clusters in batch / N_clusters_in_batch\"\n",
        "            avg_p_eta_for_batch_clusters = p_eta_for_unique_clusters_in_batch.mean()\n",
        "            prior_term = avg_p_eta_for_batch_clusters * self.l2_lambda\n",
        "\n",
        "\n",
        "            # Original code also updated self.p_eta buffer - replicate this.\n",
        "            # self.p_eta stores the quadratic form value (eta^T Sigma^-1 eta) for each cluster, not divided by 2.\n",
        "            if self.training:\n",
        "                # Create a temporary tensor of zeros with the same shape as self.p_eta\n",
        "                temp_p_eta_update = self.p_eta.clone() # Keep old values for clusters not in batch\n",
        "                temp_p_eta_update[unique_model_indices_in_batch] = p_eta_for_unique_clusters_in_batch * 2.0 # Store full quadratic form\n",
        "                self.p_eta.copy_(temp_p_eta_update)\n",
        "\n",
        "\n",
        "        # Combine terms: Negative log-likelihood\n",
        "        # Loss = -log P(Y|eta,theta) - log P(eta|theta)\n",
        "        # Likelihood term is proportional to sum_sq_err. If sigma_2 is variance, then NLL_data = 0.5 * sum_sq_err / sigma_2 + 0.5 * N_batch * log(2*pi*sigma_2)\n",
        "        # Prior term is 0.5 * eta^T Sigma^-1 eta. NLL_prior = 0.5 * eta^T Sigma^-1 eta + 0.5 * log(det(2*pi*Sigma))\n",
        "        # The constant log terms are often dropped for optimization.\n",
        "        # The loss here is MSE_batch/sigma_2 + lambda * avg(0.5 * eta^T Sigma^-1 eta)\n",
        "        # This seems like a simplified objective.\n",
        "        total_loss = likelihood_term + prior_term\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "# --- Data Loading and Preparation (R-style split) ---\n",
        "def load_and_prep_parkinsons_r_split(dtype=TENSOR_DTYPE):\n",
        "    \"\"\"\n",
        "    Loads and prepares the Parkinson's Telemonitoring dataset using an R-like\n",
        "    \"leave-last-observation-out-per-subject\" splitting strategy.\n",
        "    \"\"\"\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data\"\n",
        "    print(f\"Downloading data from {url}...\")\n",
        "    try:\n",
        "        s = requests.get(url).content\n",
        "        # The R code defines col_names, but pandas can infer from header.\n",
        "        # The UCI file has a header.\n",
        "        data = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "        print(\"Data downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading or reading data: {e}\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Rename subject column to be consistent with original Python script\n",
        "    if 'subject#' in data.columns:\n",
        "        data = data.rename(columns={'subject#': 'subject'})\n",
        "    elif 'subject' not in data.columns:\n",
        "        print(\"Error: Subject column ('subject#' or 'subject') not found.\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "    subject_col = 'subject'\n",
        "\n",
        "    # Define outcome and predictors as in the R script\n",
        "    outcome_col = 'total_UPDRS'\n",
        "    predictor_cols = [\n",
        "        \"Jitter(%)\", \"Jitter(Abs)\", \"Jitter:RAP\", \"Jitter:PPQ5\", \"Jitter:DDP\",\n",
        "        \"Shimmer\", \"Shimmer(dB)\", \"Shimmer:APQ3\", \"Shimmer:APQ5\", \"Shimmer:APQ11\",\n",
        "        \"Shimmer:DDA\", \"NHR\", \"HNR\", \"RPDE\", \"DFA\", \"PPE\"\n",
        "    ]\n",
        "\n",
        "    # Verify all predictor columns and outcome column exist\n",
        "    missing_cols = [col for col in predictor_cols + [outcome_col] if col not in data.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Error: Missing required columns: {missing_cols}\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Select relevant columns (subject, test_time for sorting, predictors, outcome)\n",
        "    # 'test_time' is crucial for identifying the \"last\" observation.\n",
        "    if 'test_time' not in data.columns:\n",
        "        print(\"Error: 'test_time' column not found, which is needed for splitting.\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Drop 'sex' column if it exists, as it's not used.\n",
        "    if 'sex' in data.columns:\n",
        "        data = data.drop(columns=['sex'])\n",
        "        print(\"Removed 'sex' column.\")\n",
        "\n",
        "    # Keep only necessary columns for processing\n",
        "    essential_cols = [subject_col, 'test_time'] + predictor_cols + [outcome_col]\n",
        "    data_subset = data[essential_cols].copy()\n",
        "\n",
        "    # Convert predictor and outcome columns to numeric, coercing errors to NaN\n",
        "    print(\"Converting predictor and outcome columns to numeric...\")\n",
        "    for col in predictor_cols + [outcome_col]:\n",
        "        data_subset[col] = pd.to_numeric(data_subset[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with any NA values in the selected predictor or outcome columns\n",
        "    initial_rows = len(data_subset)\n",
        "    data_subset.dropna(subset=predictor_cols + [outcome_col], inplace=True)\n",
        "    removed_rows = initial_rows - len(data_subset)\n",
        "    if removed_rows > 0:\n",
        "        print(f\"Removed {removed_rows} rows containing NA values in key columns.\")\n",
        "    if len(data_subset) == 0:\n",
        "        print(\"Error: No data remaining after NA removal.\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Sort data by subject and test_time to ensure \"last\" observation is truly last\n",
        "    data_subset = data_subset.sort_values([subject_col, 'test_time']).reset_index(drop=True)\n",
        "    print(f\"Data sorted by '{subject_col}' and 'test_time'.\")\n",
        "\n",
        "    # --- R-like \"Leave-Last-Observation-Out per Subject\" Split ---\n",
        "    print(\"Splitting data (last observation per subject for test)...\")\n",
        "    train_list_X_df, train_list_y_df, train_list_groups_series = [], [], []\n",
        "    test_list_X_df, test_list_y_df, test_list_groups_series = [], [], []\n",
        "\n",
        "    X_unscaled_df_all = data_subset[predictor_cols]\n",
        "    y_df_all = data_subset[[outcome_col]] # Keep as DataFrame for consistent .loc\n",
        "    groups_series_all = data_subset[subject_col]\n",
        "\n",
        "    for subject_id, group_df in data_subset.groupby(subject_col):\n",
        "        if len(group_df) < 1: # Should not happen if subject exists\n",
        "            continue\n",
        "\n",
        "        # Last observation for test set\n",
        "        test_indices = group_df.index[-1:] # Index for the last row\n",
        "        test_list_X_df.append(X_unscaled_df_all.loc[test_indices])\n",
        "        test_list_y_df.append(y_df_all.loc[test_indices])\n",
        "        test_list_groups_series.append(groups_series_all.loc[test_indices])\n",
        "\n",
        "        # All other observations for training set (if any)\n",
        "        if len(group_df) > 1:\n",
        "            train_indices = group_df.index[:-1] # Indices for all but the last row\n",
        "            train_list_X_df.append(X_unscaled_df_all.loc[train_indices])\n",
        "            train_list_y_df.append(y_df_all.loc[train_indices])\n",
        "            train_list_groups_series.append(groups_series_all.loc[train_indices])\n",
        "\n",
        "    if not train_list_X_df or not test_list_X_df:\n",
        "        print(\"Error: Training or test set is empty after splitting. \"\n",
        "              \"This might happen if subjects have only one observation.\")\n",
        "        # Check if any subject has only one observation\n",
        "        obs_counts = data_subset.groupby(subject_col).size()\n",
        "        if (obs_counts == 1).any():\n",
        "            print(f\"Warning: Some subjects have only 1 observation. These will only contribute to the test set, \"\n",
        "                  f\"leading to an empty training set for them. Subjects with 1 obs: {obs_counts[obs_counts==1].index.tolist()}\")\n",
        "        if not train_list_X_df: # If training set is completely empty\n",
        "             print(\"FATAL: Training set is completely empty. Cannot proceed.\")\n",
        "             return None, None, None, None, None, None, None, None, None, None\n",
        "        # If only test_list_X_df is empty, it might be okay if all subjects had 1 obs, but R code errors.\n",
        "        # For this script, we need a test set.\n",
        "\n",
        "    # Concatenate lists of DataFrames/Series\n",
        "    X_train_unscaled_df = pd.concat(train_list_X_df)\n",
        "    y_train_df = pd.concat(train_list_y_df)\n",
        "    groups_train_series = pd.concat(train_list_groups_series)\n",
        "\n",
        "    X_test_unscaled_df = pd.concat(test_list_X_df)\n",
        "    y_test_df = pd.concat(test_list_y_df)\n",
        "    groups_test_series = pd.concat(test_list_groups_series)\n",
        "\n",
        "    print(f\"Train size: {len(X_train_unscaled_df)} ({groups_train_series.nunique()} subjects)\")\n",
        "    print(f\"Test size: {len(X_test_unscaled_df)} ({groups_test_series.nunique()} subjects)\")\n",
        "\n",
        "    if len(X_train_unscaled_df) == 0 or len(X_test_unscaled_df) == 0 :\n",
        "        print(\"FATAL: Train or test set is empty. Aborting.\")\n",
        "        return None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "\n",
        "    # Scale features: Fit on training data, transform both train and test\n",
        "    print(\"Scaling features...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled_np = scaler.fit_transform(X_train_unscaled_df)\n",
        "    X_test_scaled_np = scaler.transform(X_test_unscaled_df)\n",
        "\n",
        "    # Convert to PyTorch tensors with specified dtype\n",
        "    X_train_tensor = torch.tensor(X_train_scaled_np, dtype=dtype)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled_np, dtype=dtype)\n",
        "    y_train_tensor = torch.tensor(y_train_df.values, dtype=dtype)\n",
        "    y_test_tensor = torch.tensor(y_test_df.values, dtype=dtype)\n",
        "    groups_train_tensor = torch.tensor(groups_train_series.values, dtype=torch.long)\n",
        "    groups_test_tensor = torch.tensor(groups_test_series.values, dtype=torch.long)\n",
        "\n",
        "    # Create meta dictionaries\n",
        "    meta_train = {'meta_id': groups_train_tensor}\n",
        "    meta_test = {'meta_id': groups_test_tensor}\n",
        "\n",
        "    # Get unique cluster IDs and counts from the training set for NME initialization\n",
        "    unique_clusters, counts = torch.unique(groups_train_tensor, return_counts=True)\n",
        "\n",
        "    print(f\"Features (X_train shape): {X_train_tensor.shape}, Target (y_train shape): {y_train_tensor.shape}\")\n",
        "    print(f\"Unique subjects in training set for NME: {len(unique_clusters)}\")\n",
        "\n",
        "\n",
        "    return X_train_tensor, y_train_tensor, meta_train, \\\n",
        "           X_test_tensor, y_test_tensor, meta_test, \\\n",
        "           unique_clusters, counts, scaler, predictor_cols # Return new predictor_cols as feature_cols\n",
        "\n",
        "\n",
        "# Helper to flatten nested lists (for parameter indices)\n",
        "def generic_flatten_nested_list(nested_list):\n",
        "    \"\"\"Flattens a nested list.\"\"\"\n",
        "    result = []\n",
        "    for element in nested_list:\n",
        "        if isinstance(element, list): result.extend(generic_flatten_nested_list(element))\n",
        "        else: result.append(element)\n",
        "    return result\n",
        "\n",
        "# --- Training and Evaluation ---\n",
        "def train_nme_model(model, X_train, y_train, meta_train, optimizer, epochs=10, batch_size=256):\n",
        "    \"\"\"Trains the NME model.\"\"\"\n",
        "    # model.train() # Set model to training mode - this is now handled by the model's train() method\n",
        "    num_samples = X_train.shape[0]\n",
        "    target_device = next(model.parameters()).device # Get device from model\n",
        "    print_freq = max(1, epochs // 20) # Print progress roughly 20 times\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time() # Start timer for epoch\n",
        "        model.train() # Ensure train mode for dropout/batchnorm if used, and NME's during_training state\n",
        "        permutation = torch.randperm(num_samples)\n",
        "        epoch_loss = 0.0\n",
        "        batches_processed = 0\n",
        "        total_random_grad_norm = 0.0\n",
        "        total_fixed_grad_norm = 0.0\n",
        "        total_shared_grad_norm = 0.0 # Add norm for shared params\n",
        "\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            # Move data to target device\n",
        "            batch_X = X_train[indices].to(target_device)\n",
        "            batch_y = y_train[indices].to(target_device)\n",
        "            batch_meta = {k: v[indices].to(target_device) for k, v in meta_train.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            scores, _ = model(batch_X, meta=batch_meta, y=batch_y)\n",
        "            # Loss calculation\n",
        "            loss = model.loss(scores, batch_y, batch_meta)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                 print(f\"Warning: NaN or Inf loss detected at epoch {epoch+1}, batch {i // batch_size}. Skipping batch.\")\n",
        "                 optimizer.zero_grad() # Zero grads even if skipping step\n",
        "                 continue # Skip optimizer step if loss is invalid\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # --- Gradient Check ---\n",
        "            if model.random:\n",
        "                batch_random_grad_norm_sq = 0.0\n",
        "                for name, param in model.random.named_parameters():\n",
        "                    if param.grad is not None:\n",
        "                        batch_random_grad_norm_sq += torch.linalg.norm(param.grad).item()**2\n",
        "                total_random_grad_norm += batch_random_grad_norm_sq # Accumulate squared norm\n",
        "            if model.fixed:\n",
        "                batch_fixed_grad_norm_sq = 0.0\n",
        "                for name, param in model.fixed.named_parameters():\n",
        "                     if param.grad is not None:\n",
        "                          batch_fixed_grad_norm_sq += torch.linalg.norm(param.grad).item()**2\n",
        "                total_fixed_grad_norm += batch_fixed_grad_norm_sq # Accumulate squared norm\n",
        "\n",
        "            # Check shared model params (from first model instance)\n",
        "            batch_shared_grad_norm_sq = 0.0\n",
        "            if model.models: # Check if models list exists\n",
        "                 first_model = model.models[0] # Access first model in ModuleList\n",
        "                 for name, param in first_model.named_parameters():\n",
        "                     std_name = name.replace(\".\", \"-\")\n",
        "                     # Check if param name (relative to submodel) is NOT random\n",
        "                     if std_name not in model.random_effect_names:\n",
        "                         if param.grad is not None:\n",
        "                             batch_shared_grad_norm_sq += torch.linalg.norm(param.grad).item()**2\n",
        "            total_shared_grad_norm += batch_shared_grad_norm_sq\n",
        "            # --- End Gradient Check ---\n",
        "\n",
        "\n",
        "            # Gradient clipping\n",
        "            # Clip grads for all parameters passed to optimizer\n",
        "            all_params = []\n",
        "            for group in optimizer.param_groups:\n",
        "                 all_params.extend(group['params'])\n",
        "            # Check if all_params is empty before clipping\n",
        "            if all_params:\n",
        "                 torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * len(indices) # Accumulate total loss\n",
        "            batches_processed += 1\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / num_samples if num_samples > 0 else 0\n",
        "\n",
        "        # Sigma updates happen via model.eval() which triggers the _after_training hook in NME\n",
        "        model.eval() # Trigger sigma update hook, then sets model.training=False\n",
        "                     # Next iteration model.train() will set model.training=True and update sigma again if needed\n",
        "\n",
        "        epoch_end_time = time.time() # End timer for epoch\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "        # Print progress less frequently for long training runs\n",
        "        if (epoch + 1) % print_freq == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_epoch_loss:.4f}, Sigma2: {model.sigma_2.item():.4f}, Duration: {epoch_duration:.2f}s\")\n",
        "            # Calculate average norms\n",
        "            avg_random_grad_norm = (total_random_grad_norm / batches_processed)**0.5 if model.random and batches_processed > 0 else 0.0\n",
        "            avg_fixed_grad_norm = (total_fixed_grad_norm / batches_processed)**0.5 if model.fixed and batches_processed > 0 else 0.0\n",
        "            avg_shared_grad_norm = (total_shared_grad_norm / batches_processed)**0.5 if batches_processed > 0 else 0.0\n",
        "\n",
        "            if model.random and hasattr(model, 'sa_sigma') and model.sa_sigma is not None and model.sa_sigma.numel() > 0 :\n",
        "                sa_sigma_cpu = model.sa_sigma.detach().cpu().numpy()\n",
        "                print(f\"  Avg Grads: Random={avg_random_grad_norm:.3e}, Fixed={avg_fixed_grad_norm:.3e}, Shared={avg_shared_grad_norm:.3e}\")\n",
        "                print(f\"  SA Sigma diag (sample): {sa_sigma_cpu[:min(5, len(sa_sigma_cpu))]}\") # Print only up to 5 or available\n",
        "                if avg_random_grad_norm < 1e-6 and epoch > 50: # Check after more initial epochs\n",
        "                     print(f\"  Warning: Low/zero gradient for random parameters!\")\n",
        "            else:\n",
        "                print(f\"  Avg Grads: Fixed={avg_fixed_grad_norm:.3e}, Shared={avg_shared_grad_norm:.3e} (No random effects or sa_sigma)\")\n",
        "\n",
        "\n",
        "def evaluate_nme_model(model, X_test, y_test, meta_test):\n",
        "    \"\"\"Evaluates the NME model.\"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    target_device = next(model.parameters()).device\n",
        "    all_preds = []\n",
        "    all_gt = []\n",
        "\n",
        "    # Move test data to device\n",
        "    X_test = X_test.to(target_device)\n",
        "    y_test = y_test.to(target_device)\n",
        "    meta_test = {k: v.to(target_device) for k, v in meta_test.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Process test set (can be done in one go if memory allows)\n",
        "        scores, _ = model(X_test, meta=meta_test, y=y_test)\n",
        "        all_preds.append(scores.cpu())\n",
        "        all_gt.append(y_test.cpu())\n",
        "\n",
        "    predictions = torch.cat(all_preds).numpy()\n",
        "    ground_truth = torch.cat(all_gt).numpy()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = np.mean((predictions - ground_truth)**2)\n",
        "    mae = np.mean(np.abs(predictions - ground_truth))\n",
        "    return mse, mae\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define dtype globally\n",
        "    model_dtype = TENSOR_DTYPE # torch.float64 for stability\n",
        "\n",
        "    # Load and prepare data with the specified R-like split\n",
        "    X_train, y_train, meta_train, \\\n",
        "    X_test, y_test, meta_test, \\\n",
        "    unique_clusters, cluster_counts, scaler, feature_cols = load_and_prep_parkinsons_r_split(dtype=model_dtype)\n",
        "\n",
        "    if X_train is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    input_size = X_train.shape[1] # This will now be 16 (number of predictor_cols)\n",
        "    output_size = y_train.shape[1] # Should be 1 for total_UPDRS\n",
        "\n",
        "    # --- Configure NME Model ---\n",
        "    # Parameter names in MLP: network.0.weight, network.0.bias, network.2.weight, ...\n",
        "    # MLP with hidden [32, 16] has layers: 0(Lin), 1(Act), 2(Lin), 3(Act), 4(Lin)\n",
        "    # Use original naming convention expected by NME author's code\n",
        "    random_effects_config = (\"network.4.bias\",) # Bias of the output layer\n",
        "    independent_config = ()\n",
        "\n",
        "    print(\"\\nConfiguring NME model (Author's version structure + Functional Forward)...\")\n",
        "    nme_model = NeuralMixedEffects(\n",
        "        clusters=unique_clusters,\n",
        "        cluster_count=cluster_counts, # Pass cluster counts\n",
        "        model_fun=MLP,\n",
        "        random_effects=random_effects_config,\n",
        "        independent=independent_config,\n",
        "        simulated_annealing_alpha=0.97,\n",
        "        dtype=model_dtype, # Pass the global dtype\n",
        "        l2_lambda=0.1, # Regularization strength for random effects prior\n",
        "        # --- MLP specific args ---\n",
        "        model_input_size=input_size,\n",
        "        model_output_size=output_size,\n",
        "        model_hidden_sizes=[32, 16], # Smaller MLP\n",
        "        model_dtype=model_dtype, # Pass dtype to MLP constructor\n",
        "        # --- NME args ---\n",
        "        input_size=input_size, # Required by NME base class\n",
        "        output_size=output_size, # Required by NME base class\n",
        "    ).to(device) # Move model to device\n",
        "\n",
        "    # --- Optimizer ---\n",
        "    # Define parameter groups for optimizer based on named_parameters logic\n",
        "    fixed_param_list = []\n",
        "    random_param_list = []\n",
        "    shared_param_list = []\n",
        "\n",
        "    # Use the model's specific named_parameters method\n",
        "    for name, param in nme_model.named_parameters():\n",
        "        if name.startswith(\"fixed.\"):\n",
        "            fixed_param_list.append(param)\n",
        "        elif name.startswith(\"random.\"):\n",
        "            random_param_list.append(param)\n",
        "        # Shared parameters are those NOT starting with fixed. or random.\n",
        "        # AND NOT part of the sub-models corresponding to random effects\n",
        "        # The named_parameters method is designed to handle this filtering\n",
        "        elif not name.startswith(\"fixed.\") and not name.startswith(\"random.\"):\n",
        "             shared_param_list.append(param)\n",
        "\n",
        "\n",
        "    optimizer_param_groups = []\n",
        "    if fixed_param_list:\n",
        "        optimizer_param_groups.append({'params': fixed_param_list, 'lr': 1e-3, 'weight_decay': 1e-5})\n",
        "    if random_param_list:\n",
        "        optimizer_param_groups.append({'params': random_param_list, 'lr': 1e-3}) # Use same LR for random for now\n",
        "    if shared_param_list:\n",
        "        optimizer_param_groups.append({'params': shared_param_list, 'lr': 1e-4, 'weight_decay': 1e-5})\n",
        "\n",
        "\n",
        "    if not optimizer_param_groups:\n",
        "         print(\"Warning: No parameters found requiring gradients for the optimizer.\")\n",
        "         # Handle this case, maybe exit or skip training\n",
        "    else:\n",
        "         optimizer = optim.Adam(optimizer_param_groups, lr=1e-3) # Default LR if only one group\n",
        "         print(\"\\nStarting training...\")\n",
        "         # *** Set epochs to 2500 ***\n",
        "         train_nme_model(nme_model, X_train, y_train, meta_train, optimizer, epochs=4000, batch_size=512)\n",
        "\n",
        "         print(\"\\nStarting evaluation...\")\n",
        "         test_mse, test_mae = evaluate_nme_model(nme_model, X_test, y_test, meta_test)\n",
        "\n",
        "         print(\"\\n--- Results ---\")\n",
        "         print(f\"Test MSE: {test_mse:.4f}\")\n",
        "         print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "         # Example of accessing final random effects (eta) for a subject\n",
        "         if nme_model.random and len(unique_clusters) > 0:\n",
        "             first_subject_id_tensor = unique_clusters[0]\n",
        "             first_subject_id = first_subject_id_tensor.item()\n",
        "\n",
        "             # Find model index corresponding to the subject ID\n",
        "             try:\n",
        "                 # Get model index from cluster_id_to_index map\n",
        "                 subject_model_idx = nme_model.cluster_id_to_index.get(first_subject_id, -1)\n",
        "\n",
        "                 if subject_model_idx != -1:\n",
        "                     print(f\"\\nRandom effect (eta) for subject {first_subject_id} (model index {subject_model_idx}):\")\n",
        "                     for name, eta_param in nme_model.random.items():\n",
        "                         # Ensure name matches the format used internally (with hyphens)\n",
        "                         std_name = name # self.random keys are already std_name\n",
        "                         print(f\"  - {std_name}: {eta_param[subject_model_idx].detach().cpu().numpy()}\")\n",
        "                 else:\n",
        "                     print(f\"Could not find subject {first_subject_id} in model cluster map.\")\n",
        "             except Exception as e:\n",
        "                print(f\"Error accessing random effect for subject {first_subject_id}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcpqC3fASOaj",
        "outputId": "d1090841-f780-49fa-fca2-72930ff9b830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data...\n",
            "Data downloaded successfully.\n",
            "Removed 'sex' column.\n",
            "Converting predictor and outcome columns to numeric...\n",
            "Data sorted by 'subject' and 'test_time'.\n",
            "Splitting data (last observation per subject for test)...\n",
            "Train size: 5833 (42 subjects)\n",
            "Test size: 42 (42 subjects)\n",
            "Scaling features...\n",
            "Features (X_train shape): torch.Size([5833, 16]), Target (y_train shape): torch.Size([5833, 1])\n",
            "Unique subjects in training set for NME: 42\n",
            "\n",
            "Configuring NME model (Author's version structure + Functional Forward)...\n",
            "Instantiating 42 models for NME...\n",
            "Instantiated NME models: <class '__main__.MLP'>\n",
            "Tracking random effect 'network-4-bias' with shape: torch.Size([42, 1]), dtype: torch.float64\n",
            "Total scalar random effects per cluster: 1\n",
            "Independent effect indices: []\n",
            "Full covariance block indices: [0]\n",
            "\n",
            "Starting training...\n",
            "Epoch 200/4000, Avg Loss: 0.9908, Sigma2: 161.8957, Duration: 0.73s\n",
            "  Avg Grads: Random=2.580e-01, Fixed=3.229e-02, Shared=3.674e-01\n",
            "  SA Sigma diag (sample): [5.11321091e-06]\n",
            "Epoch 400/4000, Avg Loss: 1.0125, Sigma2: 125.2536, Duration: 0.73s\n",
            "  Avg Grads: Random=8.031e+01, Fixed=2.095e-02, Shared=3.271e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 600/4000, Avg Loss: 0.9991, Sigma2: 118.1096, Duration: 0.71s\n",
            "  Avg Grads: Random=6.486e+01, Fixed=1.726e-02, Shared=3.145e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 800/4000, Avg Loss: 1.0105, Sigma2: 107.3056, Duration: 0.72s\n",
            "  Avg Grads: Random=6.071e+01, Fixed=1.018e-02, Shared=2.841e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 1000/4000, Avg Loss: 1.0112, Sigma2: 98.1722, Duration: 0.94s\n",
            "  Avg Grads: Random=8.929e+01, Fixed=6.126e-03, Shared=2.831e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 1200/4000, Avg Loss: 1.0081, Sigma2: 95.3627, Duration: 0.70s\n",
            "  Avg Grads: Random=6.477e+01, Fixed=7.546e-03, Shared=2.880e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 1400/4000, Avg Loss: 1.0061, Sigma2: 93.3255, Duration: 0.72s\n",
            "  Avg Grads: Random=8.265e+01, Fixed=7.681e-03, Shared=3.474e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 1600/4000, Avg Loss: 1.0105, Sigma2: 89.9617, Duration: 0.74s\n",
            "  Avg Grads: Random=6.064e+01, Fixed=1.018e-02, Shared=3.812e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 1800/4000, Avg Loss: 1.0090, Sigma2: 87.8631, Duration: 0.73s\n",
            "  Avg Grads: Random=7.295e+01, Fixed=1.162e-02, Shared=3.883e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 2000/4000, Avg Loss: 1.0176, Sigma2: 86.0929, Duration: 0.72s\n",
            "  Avg Grads: Random=9.036e+01, Fixed=8.884e-03, Shared=4.155e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 2200/4000, Avg Loss: 1.0225, Sigma2: 83.3499, Duration: 0.72s\n",
            "  Avg Grads: Random=1.044e+02, Fixed=1.006e-02, Shared=3.997e-01\n",
            "  SA Sigma diag (sample): [1.24271934e-08]\n",
            "Epoch 2400/4000, Avg Loss: 1.0202, Sigma2: 81.6106, Duration: 0.71s\n",
            "  Avg Grads: Random=9.456e+01, Fixed=1.038e-02, Shared=5.246e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 2600/4000, Avg Loss: 1.0116, Sigma2: 80.3660, Duration: 0.96s\n",
            "  Avg Grads: Random=8.123e+01, Fixed=1.215e-02, Shared=4.696e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 2800/4000, Avg Loss: 1.0054, Sigma2: 79.2995, Duration: 0.89s\n",
            "  Avg Grads: Random=7.859e+01, Fixed=8.572e-03, Shared=5.013e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 3000/4000, Avg Loss: 1.0092, Sigma2: 78.4645, Duration: 0.70s\n",
            "  Avg Grads: Random=6.785e+01, Fixed=7.977e-03, Shared=4.430e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 3200/4000, Avg Loss: 1.0037, Sigma2: 77.3604, Duration: 0.72s\n",
            "  Avg Grads: Random=6.463e+01, Fixed=1.235e-02, Shared=5.871e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 3400/4000, Avg Loss: 1.0088, Sigma2: 75.8253, Duration: 0.74s\n",
            "  Avg Grads: Random=8.061e+01, Fixed=1.146e-02, Shared=5.604e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 3600/4000, Avg Loss: 1.0066, Sigma2: 75.2267, Duration: 0.70s\n",
            "  Avg Grads: Random=7.024e+01, Fixed=1.140e-02, Shared=5.518e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 3800/4000, Avg Loss: 1.0108, Sigma2: 74.2447, Duration: 0.73s\n",
            "  Avg Grads: Random=7.066e+01, Fixed=1.168e-02, Shared=5.655e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "Epoch 4000/4000, Avg Loss: 1.0082, Sigma2: 74.0278, Duration: 0.79s\n",
            "  Avg Grads: Random=7.388e+01, Fixed=9.941e-03, Shared=6.506e-01\n",
            "  SA Sigma diag (sample): [1.e-08]\n",
            "\n",
            "Starting evaluation...\n",
            "\n",
            "--- Results ---\n",
            "Test MSE: 103.4075\n",
            "Test MAE: 8.1786\n",
            "\n",
            "Random effect (eta) for subject 1 (model index 0):\n",
            "  - network-4-bias: [1.13827537]\n"
          ]
        }
      ]
    }
  ]
}